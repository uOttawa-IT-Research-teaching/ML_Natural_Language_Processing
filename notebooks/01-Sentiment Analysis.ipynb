{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7480cb5-8dba-4600-9821-feab107fab59",
   "metadata": {},
   "source": [
    "# Natural language processing\n",
    "\n",
    "Natural language processing (NLP) is a machine learning technique to analyze large amounts of text in order to extract information. Some examples are sentiment analysis, translation, transcription, summarizing and tagging. NLP is a very broad term and can apply to anything text related.\n",
    "\n",
    "It's also a very hard task for computers because language is a very ill-defined thing. For example,\n",
    "* Verbs can sometimes be nouns (make, set)\n",
    "* Words that are spelled the same can have different meanings (spot=stain/availability, fall=season/drop).\n",
    "* Idioms (hot potato, piece of cake)\n",
    "\n",
    "Try explaining all of that to a computer! Yet, depending on the goal, that is exactly what we need to do with NLP.\n",
    "\n",
    "Comment: explain in this notebook we are going to condcut analysis on the data taken from Yelp and do \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a34d10-d918-4299-8569-59a58cd7585a",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "Average time to complete: 30min\n",
    "\n",
    "By the end of this tutorial you should be able to:\n",
    "\n",
    "* Visualize parts of the data and interpret the results of word clouds\n",
    "* Describe what Sentiment Analysis is and how we use it\n",
    "* Use the default model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6250c83-5786-4c49-892a-a531b57a233d",
   "metadata": {},
   "source": [
    "## What you will need for this tutorial\n",
    "\n",
    "* See the [introduction document](https://uottawa-it-research-teaching.github.io/machinelearning/) for general requirements and how Jupyter notebooks work.\n",
    "* We'll need Pandas for convenient data handling. It's a very powerful Python package that can read CSV and Excel files. It also has very good data manipulation capabilities which come in use for data cleaning.\n",
    "* We will use NLTK as our machine learning package. NLTK stands for Natural Language Tool Kit.\n",
    "* The data files that you will need must be downloaded from Yelp yourself\n",
    "\n",
    "One other package that you will need is \"wordcloud\" which we will use later to create word clouds. This is not a package that is usually installed by default, so you will need to install it manually. How to do so, depends on your Python environment. If you use Anaconda or Miniconda, it would be:\n",
    "\n",
    "    conda install wordcloud\n",
    "\n",
    "For most other Python environments, it would be:\n",
    "\n",
    "    pip install wordcloud\n",
    "\n",
    "Comment: the wordcloud is being installed successfully. just make sure to include step by step process of instalation here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904f74f-5f76-405d-98df-28589531950e",
   "metadata": {},
   "source": [
    "## RDM best practices\n",
    "\n",
    "Good data handling for machine learning begins with good Research Data Management (RDM). The quality of your source data will impact the outcome of your results, just like the reproducibility of your results will depend on the quality of your data sources, in addition to how you organize the data so that other people (and machines!) can understand and reuse it. \n",
    "\n",
    "We also need to respect a few research data management best practices along the way, these best practices are recommended by the [Digital Research Alliance of Canada](https://zenodo.org/records/4000989).\n",
    "\n",
    "SAVE YOUR RAW DATA IN ORIGINAL FORMAT\n",
    "* Don't overwrite your original data with a cleaned version.\n",
    "* Protect your original data by locking them or making them read-only.\n",
    "* Refer to this original data if things go wrong (as they often do).\n",
    "\n",
    "BACKUP YOUR DATA\n",
    "* Use the 3-2-1 rule: Save three copies of your data, on two different storage mediums, and one copy off site. The off site storage can be OneDrive or Google drive or whatever your institution provides.\n",
    "* We are using Open Data, so it does not contain any personally identifiable data or data that needs to be restricted or protected in any way. However, if your data contains confidential information, it is important to take steps to restrict access and encrypt your data.\n",
    "\n",
    "There are a few more RDM best practices that will help you in your project management, and we will highlight them at the beginning of each tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034dedb6-9eb8-491a-91fc-b1ef9097dbfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import json\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bed4a7-9f99-4a21-b946-8eb992b89017",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "People often have opinions when studying a controversial topic. From online discourse, you can glean a lot of information, but it requires reading every post to see if people feel positive or negative about the topic. NLP can help with this. Using the Natural Language Toolkit (NLTK), you will be able to do sentiment analysis.\n",
    "\n",
    "Sentiment analysis works by creating frequency distributions that correspond to positive texts and distributions that correspond negative text. Then, using those distributions, it is able to tell whether a new text is closer to one distribution than another and thus mark a text as positive or negative.\n",
    "\n",
    "Yelp provides a free dataset that has a big chunk of their business reviews with over 6 million reviews. This is an excellent dataset for sentiment analysis. It is, however, also really big, so you will have to download it from the website yourself. It's important to note that the data has a specific Dataset licence you must agree to, which details what you are and are not allowed to do with it.\n",
    "\n",
    "Download it here: https://www.yelp.com/dataset\n",
    "\n",
    "The file you download will be called \"Yelp-JSON.zip\". You will need to extract the files and save them somewhere on your computer. The ZIP file contains a directory called \"Yelp JSON\" with two files:\n",
    "\n",
    "* Yelp Dataset Documentation & ToS copy.pdf\n",
    "* yelp_dataset.tar\n",
    "\n",
    "The data is contained in the file called \"yelp_dataset.tar\". A TAR file is like a ZIP file in that it needs to be extracted. Both Linux and macOS can handle this file type natively, but for Windows you need a 3rd party tool like [7-zip](https://www.7-zip.org/) or [WinZip](https://www.winzip.com/en/download/winzip/).\n",
    "\n",
    "**Note:** In this notebook, we extract the data to \"/tmp/yelp\" but you will likely have a different path. Adjust any `open` statement below to point to wherever you have stored the json files on your computer.\n",
    "\n",
    "The dataset is made up of multiple files. Business details and reviews are in different files. The records in those files are all linked with ID numbers though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f873ba-b6b5-4368-b01e-beba3abe9264",
   "metadata": {},
   "source": [
    "The data is in JSONL format, which is just a text file containing lots of JSON records with one JSON record on each line. JSON in turn is a markup language that is great at representing unstructured data in a textual format. Python has the JSON library to deal with that. Let's read the first business. Business information is stored in \"yelp_academic_dataset_business.json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93164d-c744-4dde-8eac-64e37eb9b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/tmp/yelp/yelp_academic_dataset_business.json') as f:\n",
    "    for line in f:\n",
    "        business = json.loads(line)\n",
    "        print(business)\n",
    "        break # This break statement breaks out of the for loop so only the first line of the JSONL file is read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f8123-bdca-4b7f-a7b8-15d1b263ca6c",
   "metadata": {},
   "source": [
    "The JSON library converted the data into a Python dictionary. You can see the various fields like \"business_id\", \"name\", \"address\" and so on. We want to use the \"business_id\" next, so we store that in the variable called `business`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74829f0d-94aa-47c9-98c3-d5dbb6f5e998",
   "metadata": {},
   "source": [
    "Our business is Abby Rappoport. Let's find the reviews that go with that business, which are stored in the list called \"yelp_academic_dataset_review.json\". That file contains 6 milion reviews from all sorts of businesses. We only want the ones for Abby, but to find them we still need to parse the entire file. So, this will take a few minutes.\n",
    "\n",
    "The reviews are stored using the \"business_id\" that we saw when reading \"yelp_academic_dataset_business.json\". We need to use that ID and compare it to the one used in the reviews file to filter out those reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c68a7-b5ac-41cc-be26-7446859cbcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "with open('/tmp/yelp/yelp_academic_dataset_review.json') as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        if review['business_id'] == business['business_id']:\n",
    "            reviews.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28606c4-d133-4835-a173-6ca819e6af8e",
   "metadata": {},
   "source": [
    "How many reviews did we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee85183d-a1ee-4e00-8ad7-05c7a11947d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4093b32-95b7-46ec-b503-215f2400d196",
   "metadata": {},
   "source": [
    "So we have 7 reviews. Let's see what they say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd8eda-4fe6-4e0d-b3d1-b5108c6d4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8c13d-3398-41a0-91b0-905e32d56a18",
   "metadata": {},
   "source": [
    "One nice way to visualize reviews (or other texts) is by creating a wordcloud.\n",
    "\n",
    "To do that we first need to merge all of the reviews into one big text. The following code will take all the \"text\" entries in the array and join them with newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566911e-fb70-471b-879e-c63e36b8bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewtext = \"\\n\".join([review['text'] for review in reviews])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d08d52-c0ef-42e9-a76e-7917e26ba033",
   "metadata": {},
   "source": [
    "Then create the wordcloud using the wordcloud package to plot it. This generates an image that we store in the variable `cloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd25d42-f35e-43e8-b341-dfa11ff83218",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = wordcloud.WordCloud(background_color='white').generate(reviewtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7200f-1ece-4575-b7d5-cf610219ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a55061-779d-4084-ab2e-bc36cf8475e6",
   "metadata": {},
   "source": [
    "The wordcloud shows us, at a glance, what this business is about. You can see that the name Abby features prominently of course, but you also see that this is a place that does acupuncture and treatments, especially for back pain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00379e-fc90-46c9-a2a3-08d8a26c4b64",
   "metadata": {},
   "source": [
    "To see if a business is good or not, we can use the stars that people usually give with a review, but this is not always very accurate. People can be very positive in their review but still not give five stars, or people can be a bit unhappy about one minor thing and go straight to giving one star. To get a better idea of client satisfaction with a business, you'd generally read the actual reviews.\n",
    "\n",
    "However, if you have lots of businesses and reviews, that will become very time consuming! So it's better to have a computer to do this. Of course, NLP is hard because language is such an ambiguous thing as mentioned earlier.\n",
    "\n",
    "Enter machine learning! NLTK has a bunch of pretrained models. One of them is VADER which is good for short texts or sentences. The `SentimentIntensityAnalyzer` will use this model to analyze the reviews.\n",
    "\n",
    "We first need to download it, but NLTK has a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c76041-f2c2-4989-8dc4-7666ec1890a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db7b36-78f3-48e5-8a43-137de751ae5d",
   "metadata": {},
   "source": [
    "Then create the analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f95b7-56f0-45ee-a429-cea8d92e3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd69c6-8494-4f7a-8e26-4d8af846244c",
   "metadata": {},
   "source": [
    "Let's see what it thinks of the first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3a302-3826-4ef6-b875-aeee9cf8aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(reviews[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0785ef4-add0-4f7b-9159-fba5b69c89fc",
   "metadata": {},
   "source": [
    "The number to look for here is compound which combines that negativity, positivity, and neutrality in one easily understandable score that ranges from -1 (BAD!) to 1 (GREAT!).\n",
    "\n",
    "So in this case, the first review is very close to 1. This must be a very positive review. Let's see what it says."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f4b57-ce66-4a94-b955-d5eb4505d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a29ad-b54d-4dd6-8434-e0ce2aaeab68",
   "metadata": {},
   "source": [
    "And indeed, that was a very positive review. Let's look at all the reviews now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1194de0-c548-4eb1-9bf6-20d5869e9c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, review in enumerate(reviews):\n",
    "    print(i, ':', sia.polarity_scores(review['text'])['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102d829-70e8-470d-a039-a506e2dc12de",
   "metadata": {},
   "source": [
    "There is one review with index 5 that is pretty negative compared to the others. Let's see what that is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8870ff4-bf69-4581-afb6-f91707975e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews[5]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f8cfd-51c2-4a7a-9540-6acd691e6cd7",
   "metadata": {},
   "source": [
    "So this was actually a positive review for Abby. However, the language itself is clearly very negative with words like \"stress\", \"debilitating\", \"bad bout\", etc. This example shows that this model isn't always correct. It is however often good enough. Even in this case, based on all of the reviews, it would still classify Abby's business as a very positive experience which is accurate.\n",
    "\n",
    "We can now combine the codes so far into one bigger code that will analyse the sentiments of multiple businesses. We'll read the first five and get the corresponding reviews for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7988c-bc27-4e86-ba52-e29162ab604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = {}\n",
    "with open('/tmp/yelp/yelp_academic_dataset_business.json') as businessfile, open('/tmp/yelp/yelp_academic_dataset_review.json') as reviewfile:\n",
    "    # Read the first businesses into a dictionary. The dictionary key is the ID of the business.\n",
    "    for i, line in enumerate(businessfile):\n",
    "        business = json.loads(line)\n",
    "        businesses[business[\"business_id\"]] = {\n",
    "            \"name\": business[\"name\"],\n",
    "            \"reviews\": [],\n",
    "            \"sentiments\": []\n",
    "        }\n",
    "        if i >= 4: # Get the first five\n",
    "            break\n",
    "\n",
    "    # Now go through all the reviews and pick the ones that have a business_id that matches any of the businsses we selected.\n",
    "    for line in reviewfile:\n",
    "        review = json.loads(line)\n",
    "        for business_id in businesses.keys():\n",
    "            if review['business_id'] == business_id:\n",
    "                businesses[business_id][\"reviews\"].append(review['text'])\n",
    "                businesses[business_id][\"sentiments\"].append(sia.polarity_scores(review['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29642c02-371b-4f70-a780-2048560c7ac5",
   "metadata": {},
   "source": [
    "The variable `businesses` now contains five businesses with their names and all of the reviews and their sentiment analysis. Now we can see how positive people are about these businesses without relying on stars or anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853851f5-97af-430f-96d7-2d6c9750dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for business in businesses.values():\n",
    "    positive = 0\n",
    "    total = 0\n",
    "    for sentiment in business[\"sentiments\"]:\n",
    "        total += 1\n",
    "        if sentiment[\"compound\"] > 0:\n",
    "            positive += 1\n",
    "    print(f\"{business['name']} scores {positive} out of {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0875af-878e-4cb6-8899-da2b8bcbf7a5",
   "metadata": {},
   "source": [
    "We could now do this for the entire dataset, but with so many businesses this is going to take a long time. If you are really interested, you can remove the `if >= 4` limit we have and just let it run for a few days!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857ef632-2d8c-49e8-98ed-83f0d43c0bba",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have seen how sentiment analysis can be used to quantify potentially huge amounts of text to get a sense if people have positive or negative opinions using the built-in pre-trained models that come with NLTK.\n",
    "\n",
    "You can also train your own models if the default ones are not good enough for your use case. That material goes beyond the scope of these notebooks, but a good tutorial on this can be found at https://realpython.com/python-nltk-sentiment-analysis/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f4286-1056-44f6-965c-3d6db9c07e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
