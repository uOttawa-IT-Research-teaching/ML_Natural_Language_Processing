{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e970877b-11f7-49ff-ad71-412f8b298055",
   "metadata": {},
   "source": [
    "# Unsupervised NLP\n",
    "\n",
    "Unsupervised Natural Language Processing (NLP) is the art of doing NLP without specifying any labels at all. The machine learning algorithm will find the groups on its own. It's a very useful technique if you do not know in advance what common themes there are in a corpus of documents. It allows you to find patterns that you cannot easily see yourself due to the large number of documents you want to study for example.\n",
    "\n",
    "Comment: Add a little bit of explanation about what we are going to do and see in this tutorial and on which data sets we are going to run the analysis. Also, you used the same acronym for the supervised in notebook 1 and unsupervised processing here; is it a typo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2635976-d682-48e0-9355-d9102881c99b",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "Average time to complete: 60min\n",
    "\n",
    "By the end of this tutorial you should be able to:\n",
    "\n",
    "* Clean your data and describe why this is important for machine learning\n",
    "* Reduce words to their core meaning\n",
    "* Apply Latent Dirichlet Allocation to create groups\n",
    "* Interpret those groups using a variety of different ways of looking at the data\n",
    "* Describe what the Unsupervised Learning is and how we use it\n",
    "* Describe what Natural Language Processing is and how to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf7e12-e9f0-4b35-8127-f89e21284a26",
   "metadata": {},
   "source": [
    "## What you will need for this tutorial\n",
    "\n",
    "* See the [introduction document](https://uottawa-it-research-teaching.github.io/machinelearning/) for general requirements and how Jupyter notebooks work.\n",
    "* We'll need Pandas for convenient data handling. It's a very powerful Python package that can read CSV and Excel files. It also has very good data manipulation capabilities which come in use for data cleaning.\n",
    "* We will use NLTK as our machine learning package.\n",
    "* The data files that should have come with this notebook.\n",
    "\n",
    "Comment: I cannot see any data files in the main folder of NLP. Plz add them along with their licenses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36945106-a028-4e93-80e0-7a076ff960b4",
   "metadata": {},
   "source": [
    "## RDM best practices\n",
    "\n",
    "Good data handling for machine learning begins with good Research Data Management (RDM). The quality of your source data will impact the outcome of your results, just like the reproducibility of your results will depend on the quality of your data sources, in addition to how you organize the data so that other people (and machines!) can understand and reuse it. \n",
    "\n",
    "We also need to respect a few research data management best practices along the way, these best practices are recommended by the [Digital Research Alliance of Canada](https://zenodo.org/records/4000989).\n",
    "\n",
    "SAVE YOUR RAW DATA IN ORIGINAL FORMAT\n",
    "* Don't overwrite your original data with a cleaned version.\n",
    "* Protect your original data by locking them or making them read-only.\n",
    "* Refer to this original data if things go wrong (as they often do).\n",
    "\n",
    "BACKUP YOUR DATA\n",
    "* Use the 3-2-1 rule: Save three copies of your data, on two different storage mediums, and one copy off site. The off site storage can be OneDrive or Google drive or whatever your institution provides.\n",
    "* We are using Open Data, so it does not contain any personally identifiable data or data that needs to be restricted or protected in any way. However, if your data contains confidential information, it is important to take steps to restrict access and encrypt your data.\n",
    "\n",
    "There are a few more RDM best practices that will help you in your project management, and we will highlight them at the beginning of each tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3d7b4-9dbe-4c85-a586-ec9c1fcf8b4b",
   "metadata": {},
   "source": [
    "## Cleaning data for NLP\n",
    "\n",
    "There are a number of things you typically need to do to clean up data meant for NLP. It does however depend on the task you want to do.\n",
    "\n",
    "* Removing stopwords\n",
    "* Removing punctuation\n",
    "* Lemmatization\n",
    "* Synonym substitution\n",
    "\n",
    "Stopwords are words that are so common that they appear everywhere. These are words like \"a\", \"the\", \"is\", and \"have\". Thus they hold no value when trying to categorize text. You instead want to focus on words that are more or less unique to the categories. English in particular also suffers from contractions like \"you'll\" and such which need to be expanded first.\n",
    "\n",
    "Punctuation doesn't add much so we can get rid of that too.\n",
    "\n",
    "Lemmatization is the technique of reducing conjugations of verb to their base form as well as removing plurals. For example, \"buy\", \"buys\", and \"bought\" are all the same verb but for computers they are different. By substituting all of these conjugations to just \"buy\", the computer can recognize them as having the same meaning.\n",
    "\n",
    "Synonyms are just different words meaning the same thing. Ideally, you want \"help\" and \"aid\" to be counted as the same word. This is a very tricky one though because some synonyms are not always exactly the same or can have different alternate meaning. So you may or may not want to do this one.\n",
    "\n",
    "NLP packages usually come with a list of stopwords of their own. They are not always the same though. There is quite a bit of ambiguity as to what constitutes a stopword. NLTK, SpaCy, Gensim, and Scikit-learn are all Python packages that have different lists. Here, we will use NLTK.\n",
    "\n",
    "\n",
    "Comment: update the info below plz. \n",
    "\n",
    "We need the \"contractions\" package which again may not be installed be default. So similar to the first notebook, we need to install that with conda:\n",
    "\n",
    "    conda install contractions\n",
    "\n",
    "or pip:\n",
    "\n",
    "    !pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d04420-4f4f-49d3-812e-a20d8812a536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\far4\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\far4\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\far4\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Requirement already satisfied: anyascii in c:\\users\\far4\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dada4488-c64f-4c30-aed7-417fec71f2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FAR4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\FAR4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import contractions\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d129e-e59c-4b8a-993c-8c49cbfab8bf",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "These are the first ten words in the English and French stopword lists:\n",
    "\n",
    "Comment: I am getting error not finding the resource stopword not found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ad6b63-6740-4341-988f-24d6e1f49793",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\FAR4/nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\FAR4/nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m10\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\FAR4/nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\FAR4\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b16130e-737f-4b8f-9095-3d0177a00dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b01fb-ab19-43aa-9142-3e12756a4361",
   "metadata": {},
   "source": [
    "You'll notice that all of the words are lower case too. It's good practice to make all words lower case, so you don't have to worry about case sensitivity when comparing words. So let's filter this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3d6cfe-af5e-4fb5-bf34-c89600737097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notice words lower case . good practise make words lower case , worry case sensitivity comparing words . let us filter sentence .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"You'll notice that all of the words are lower case too. It's good practise to make all words lower case, so you don't have to worry about case sensitivity when comparing words. Let's filter this sentence.\"\n",
    "\n",
    "# Make it all lower case\n",
    "text = text.lower()\n",
    "\n",
    "# Remove contactions\n",
    "text = contractions.fix(text)\n",
    "\n",
    "# Tokenize the text which splits the words and punctiation\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove the stopwords and make everything lower case\n",
    "filtered_text = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "# Show result in a readable way\n",
    "\" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602bc7e-4795-401b-b779-df41454bc540",
   "metadata": {},
   "source": [
    "### Removing punctuation\n",
    "\n",
    "It is fairly easy to do using the `isalpha` function which keeps only things that actually contain letters. So digits and punctuation will disappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6aea04f-aff9-4843-a74f-a09787e6d9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notice words lower case good practise make words lower case worry case sensitivity comparing words let us filter sentence'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token for token in filtered_text if token.isalpha()]\n",
    "\n",
    "# Show result in a readable way\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17ba98-10c0-4dd6-ac28-85e2735c734d",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Unfortunately, this one is limited in the NLTK to English only. There is however a separate project for a French version which works as a drop-in for the one from NTLK. It's called [LEFF Lematizer](https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer).\n",
    "\n",
    "Comment: are we using this one and talking about it here? if not, please specify where learners can go and get the necessary training on that.\n",
    "comment: explain what the code below does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ff4a3e-2b61-4af0-a4ff-e1734fa45720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notice word lower case good practise make word lower case worry case sensitivity comparing word let u filter sentence'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90cd0a-246e-46ca-b592-cf1f5425d3a0",
   "metadata": {},
   "source": [
    "However, there is something odd going here! It's keeping \"comparing\" even that should have been \"compare\". The reason is that WordNet assumes that everything is a noun unless told otherwise.\n",
    "\n",
    "comment: and also the code below. what this one does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d232557-23eb-47f4-b6a8-752b820e95b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comparing'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('comparing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0d342-ca4c-4c4e-b27f-db2807cf96d6",
   "metadata": {},
   "source": [
    "This is interpreted by NLTK as \"The Comparing\" so it leaves it alone. But we want to mark it as a verb so it can conjugate the word back to the base form.\n",
    "\n",
    "Comment: explain the function of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f5ff15d-1dd0-4289-9161-f0cfac6a59e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'compare'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same code but now marked as verb\n",
    "lemmatizer.lemmatize('comparing', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582220b0-94e8-4a34-9892-0eeb132c4dc4",
   "metadata": {},
   "source": [
    "Fortunately, we don't need to do the tagging of words ourselves. We can use `pos_tag` to do it for us.\n",
    "\n",
    "comment: what do you mean from tagging of words, please explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed92b29e-0ca2-4f05-95a2-2b5776dc4738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('notice', 'NN'),\n",
       " ('word', 'NN'),\n",
       " ('lower', 'JJR'),\n",
       " ('case', 'NN'),\n",
       " ('good', 'JJ'),\n",
       " ('practise', 'NN'),\n",
       " ('make', 'VBP'),\n",
       " ('word', 'NN'),\n",
       " ('lower', 'JJR'),\n",
       " ('case', 'NN'),\n",
       " ('worry', 'NN'),\n",
       " ('case', 'NN'),\n",
       " ('sensitivity', 'NN'),\n",
       " ('comparing', 'VBG'),\n",
       " ('word', 'NN'),\n",
       " ('let', 'NN'),\n",
       " ('u', 'JJ'),\n",
       " ('filter', 'NN'),\n",
       " ('sentence', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91dd2fd-8e88-4b74-947f-93249b03253d",
   "metadata": {},
   "source": [
    "It classified all of the words using a pretrained model that comes with NLTK. It's possible to specify language as well with `lang='fra'` for classifying in French for example.\n",
    "\n",
    "The codes that it spits out like `NN` and `VBG` means \"Noun, singular or mass\" and \"Verb, gerund or present participle\", respectively. It depends on what tagset you use though. The default English one in NLTK uses the [Penn Treebank tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "comment: are we using the tagset you provided link above? if yes, please especify in the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da9297-2787-42ae-9f67-b4d64986af53",
   "metadata": {},
   "source": [
    "For the tagging in WordNet, these codes need to be translated. We can write a function for that. For some reason, this does not seem to be built-in with NTLK. Possibly because the tags change with different tagsets. However, we can just look at the first letter of the tag and use that. The function to translate from one tagging system to the other then is:\n",
    "\n",
    "Comment: consider adding more elaboration on the sentence above after however. Maybe add an example to clarify what you mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c29e881f-2c99-448c-b156-bdf737dacac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    elif tag.startswith('S'):\n",
    "        return nltk.corpus.wordnet.ADJ_SAT\n",
    "    else:\n",
    "        # If it's something else, then just use the default value for the lemmatizer\n",
    "        return nltk.corpus.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33880df6-0c1c-4371-bb95-9b880db61064",
   "metadata": {},
   "source": [
    "Technically, it is better to do the tagging before removing the punctuation and do the tagging on sentences because punctuation has semantic meaning that the tagger can use, but this was a better order for educational purposes. In the real example further down, we'll do it in the proper order. Anyway, let's see what we get with the tagged words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42afe0f2-083f-4098-a2a8-96a484614a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notice word low case good practise make word low case worry case sensitivity compare word let u filter sentence'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in nltk.pos_tag(tokens)]\n",
    "\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa587545-d18e-477a-bc29-34883f27951a",
   "metadata": {},
   "source": [
    "Almost every word has now been standardized to its root. It also kind of sounds like a pirate now.\n",
    "\n",
    "Comment: in the result above, i can see a standalone u. is it a mistake? if yes please correct it. if not, can we see u as a standadized root?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a071f43-a76a-4bba-b03f-7291089153c4",
   "metadata": {},
   "source": [
    "## Back to the books\n",
    "\n",
    "We'll use the CMU books dataset that we used before with KNN and Bayes tutorials. This time, we will use an unsupervised classification algorithm to see what it can find. Unsupervised means that we will not be using any of the labels that come with the dataset but instead we let our machine learning algorithm figure things out for itself. It will try to find book summaries that can be grouped together and then tell us why it thought so.\n",
    "\n",
    "However, since unsupervised learning does not have any understanding of the material, it is still up to you to figure out what the genres it found are.\n",
    "\n",
    "### Reading the data\n",
    "In the same way as in previous material for KNN, we use Pandas and JSON for loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4888ad9-c8c5-4958-9766-c86ce1e36340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f04635c9-c24d-4af1-a4a7-da8d086ca049",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv('../data/booksummaries.txt', sep=\"\\t\", header=0, names=['wikipedia', 'freebase', 'title', 'author', 'publicationdate', 'genres', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ed2b997-25f2-4d20-9964-0dee2864018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8f051a9-8929-448b-a628-b6fb87530df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre(row):\n",
    "    g = json.loads(row.genres)\n",
    "    return list(g.values())\n",
    "\n",
    "#genresperbook = books.apply(genre, axis=1)\n",
    "#books = books.assign(genres=genresperbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eec9de-bbae-4ab5-8a52-3d9178f9e797",
   "metadata": {},
   "source": [
    "You'll notice that we still expressly commented out the reading genres in our Pandas DataFrame. We won't use it.\n",
    "\n",
    "### Preparing the data\n",
    "\n",
    "Let's follow the NLP steps as we did before using the first book. This time, we'll start with the tagging before we remove the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8986f717-aed1-4b05-a1ff-999f83318898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Clockwork Orange'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which book are we looking at?\n",
    "books.loc[0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d60d8a-5504-4f3b-9db1-4cdf0cb3316e",
   "metadata": {},
   "source": [
    "Let's get started! We change everything to lowercase and we also remove the contractions. Then we tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9892442e-c3f0-4d9b-896d-6ee9d68bc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it all lower case\n",
    "text = books.loc[0]['summary'].lower()\n",
    "\n",
    "# Remove contactions\n",
    "text = contractions.fix(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08276414-85c5-482b-9e0e-b37b58bffd20",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now do the tagging of words and then feed that into the lemmatizer to get to the base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59527d0a-4259-4945-82a9-11a988ca03e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in nltk.pos_tag(tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4f45f-cf0c-45bc-9d63-3f9c6ecee2bc",
   "metadata": {},
   "source": [
    "How many tokens do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8684551a-b361-4073-8b28-c55fe518a464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1149"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72564f42-36e9-46e9-9001-a9ca57137453",
   "metadata": {},
   "source": [
    "Remove the stop words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10b2415-67c4-43a1-b78a-0747d46da757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation\n",
    "tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "# Stopwords\n",
    "tokens = [token for token in tokens if token not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e7148-35e8-4966-8753-7413fb5f0607",
   "metadata": {},
   "source": [
    "Well, that likely got rid of some junk! Let's see how many tokens we are left with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "937fcc38-13b6-49ce-836f-8aefcd644519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e89ad-16b0-42f3-867d-b8123767eb22",
   "metadata": {},
   "source": [
    "We can temporarily join the remaining tokens back to get a readable text again. This is just to see what a filtered book summary looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a1e4e29-54e6-416c-a613-a273f0b864fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alex teenager living england lead gang nightly orgy opportunistic random alex friend droogs novel slang nadsat dim bruiser gang muscle georgie ambitious pete mostly play along droogs indulge taste characterize sociopath harden juvenile delinquent alex also intelligent sophisticated taste music particularly fond beethoven lovely ludwig van novel begin droogs sit favorite hangout korova milkbar drink cocktail call hype night mayhem assault scholar walk home public library rob store leave owner wife bloody unconscious stomp panhandling derelict scuffle rival gang joyride countryside stolen car break isolated cottage maul young couple living beat husband rap wife metafictional touch husband writer work manuscript call clockwork orange alex contemptuously read paragraph state novel main theme shred manuscript back milk bar alex punishes dim crude behaviour strain within gang become apparent home dreary flat alex play classical music top volume fantasizing even orgiastic violence alex skips school next day follow unexpected visit deltoid advisor alex meet pair girl take back parent flat administer hard drug rap even alex find droogs mutinous mood georgie challenge alex leadership gang demand pull job alex quell rebellion slash dim hand fighting georgie show generosity take bar alex insists follow georgie idea burgle home wealthy old woman start farce end tragic pathos alex attack kill elderly woman escape block dim attack alex leave incapacitate front step police arrive sentence prison murder alex get job wing chapel play religious music stereo service well singing hymn prison chaplain mistake alex bible study stirring faith alex actually reading scripture violent passage alex fellow cellmates blame beat troublesome cellmate death agree undergo experimental treatment call ludovico technique technique form aversion therapy alex receive injection make feel sick watch graphically violent film eventually condition suffer cripple bout nausea mere thought violence unintended consequence soundtrack one fifth alex unable listen beloved classical music effectiveness technique demonstrate group vip watch alex collapse walloping bully abase young woman whose presence arouse predatory sexual inclination though prison chaplain accuse state strip alex free government official scene please result alex release society since parent rent room lodger alex wanders street enters public library hop learn painless way commit suicide accidentally encounter old scholar assault earlier book keen revenge beat alex help friend policeman come alex rescue turn none dim former gang rival billyboy two policeman take alex outside town beat daze bloody alex collapse door isolated cottage realize late house droogs invade first half story gang wear mask assault writer recognize alex writer whose name reveal alexander shelter alex question conditioning sequence reveal alexander die injury inflict husband decide continue live fragrant memory persists despite horrid memory alexander critic government hop use alex symbol state brutality thereby prevent incumbent government eventually begin realize alex role happening night two year ago one alexander radical associate manages extract confession alex remove alexander home lock flatblock near former home alex subject relentless barrage classical music prompt attempt suicide leap high window alex wake hospital court government official anxious counter bad publicity create suicide attempt alexander safely pack mental institution alex offer job agree side government photographer snap picture alex daydream orgiastic violence realize ludovico conditioning reverse cure right final chapter alex new trio droogs find begin outgrow taste violence chance encounter pete married settle inspires alex seek wife family contemplate likelihood future son delinquent prospect alex view fatalistically'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810f6aa-a7b2-40c7-a6e2-253e113631e8",
   "metadata": {},
   "source": [
    "That seemed to have worked. We got a nice word salad. One thing we might want to do is also to filter out names, but let's not overcomplicate things! If you are interested, search for NLTK and NER (Named Entity Recognizer).\n",
    "\n",
    "Now we can do this for all of the books. We'll put all of the above in a single function that we can then apply to all of the rows. This will take a few minutes. There are a lot of books (9292)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41a6fb42-af3d-48d2-ade4-bc277068d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(text):\n",
    "    # Make it all lower case and remove contactions\n",
    "    text = contractions.fix(text.lower())\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in nltk.pos_tag(tokens)]\n",
    "\n",
    "    # Punctuation\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92a1e13a-60ea-4a33-92e3-cee8de917c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the prepare function to all of the books and store it in a new column.\n",
    "books['prepared'] = books['summary'].apply(prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17288170-3b47-4603-b476-ef975f4499be",
   "metadata": {},
   "source": [
    "This previous step takes a long time. In such cases, you might want to just save the prepared data into a file that you can then just read without needing to run all of the preparation steps again. This can be done with `to_pickle` and `read_pickle`. Pickle is a Python specific format that is fast and dense. It's not just for DataFrames either, any Python variable can be stored in a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ada487fe-3df0-4168-91ec-babf9a1c47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prepared DataFrame to file\n",
    "books.to_pickle('books.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e252b60-2fce-4b68-a80c-5b52df3d3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the prepared Data Frame\n",
    "books = pd.read_pickle('books.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6a59b-7511-4974-8005-a3dae242e1c1",
   "metadata": {},
   "source": [
    "We now have a new column in our `books` DataFrame that contains the filtered clean text as a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c90ca027-0f0a-4c27-b310-b4a4f9f26ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wikipedia</th>\n",
       "      <th>freebase</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publicationdate</th>\n",
       "      <th>genres</th>\n",
       "      <th>summary</th>\n",
       "      <th>prepared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>843</td>\n",
       "      <td>/m/0k36</td>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Anthony Burgess</td>\n",
       "      <td>1962</td>\n",
       "      <td>{\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "      <td>[alex, teenager, living, england, lead, gang, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>986</td>\n",
       "      <td>/m/0ldx</td>\n",
       "      <td>The Plague</td>\n",
       "      <td>Albert Camus</td>\n",
       "      <td>1947</td>\n",
       "      <td>{\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "      <td>[text, plague, divide, five, part, town, oran,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2152</td>\n",
       "      <td>/m/0x5g</td>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>Erich Maria Remarque</td>\n",
       "      <td>1929-01-29</td>\n",
       "      <td>{\"/m/098tmk\": \"War novel\", \"/m/016lj8\": \"Roman...</td>\n",
       "      <td>The book tells the story of Paul Bäumer, a Ge...</td>\n",
       "      <td>[book, tell, story, paul, bäumer, german, sold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2890</td>\n",
       "      <td>/m/011zx</td>\n",
       "      <td>A Wizard of Earthsea</td>\n",
       "      <td>Ursula K. Le Guin</td>\n",
       "      <td>1968</td>\n",
       "      <td>{\"/m/0dwly\": \"Children's literature\", \"/m/01hm...</td>\n",
       "      <td>Ged is a young boy on Gont, one of the larger...</td>\n",
       "      <td>[ged, young, boy, go, one, large, island, nort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4081</td>\n",
       "      <td>/m/01b4w</td>\n",
       "      <td>Blade Runner 3: Replicant Night</td>\n",
       "      <td>K. W. Jeter</td>\n",
       "      <td>1996-10-01</td>\n",
       "      <td>{\"/m/06n90\": \"Science Fiction\", \"/m/014dfn\": \"...</td>\n",
       "      <td>Living on Mars, Deckard is acting as a consul...</td>\n",
       "      <td>[live, mar, deckard, act, consultant, movie, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16548</th>\n",
       "      <td>36372465</td>\n",
       "      <td>/m/02vqwsp</td>\n",
       "      <td>The Third Lynx</td>\n",
       "      <td>Timothy Zahn</td>\n",
       "      <td>2007</td>\n",
       "      <td>{\"/m/06n90\": \"Science Fiction\"}</td>\n",
       "      <td>The story starts with former government agent...</td>\n",
       "      <td>[story, start, former, government, agent, fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16550</th>\n",
       "      <td>36534061</td>\n",
       "      <td>/m/072y44</td>\n",
       "      <td>Remote Control</td>\n",
       "      <td>Andy McNab</td>\n",
       "      <td>1997</td>\n",
       "      <td>{\"/m/01jfsb\": \"Thriller\", \"/m/02xlf\": \"Fiction...</td>\n",
       "      <td>The series follows the character of Nick Ston...</td>\n",
       "      <td>[series, follow, character, nick, stone, man, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16554</th>\n",
       "      <td>37054020</td>\n",
       "      <td>/m/04f1nbs</td>\n",
       "      <td>Transfer of Power</td>\n",
       "      <td>Vince Flynn</td>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>{\"/m/01jfsb\": \"Thriller\", \"/m/02xlf\": \"Fiction\"}</td>\n",
       "      <td>The reader first meets Rapp while he is doing...</td>\n",
       "      <td>[reader, first, meet, rapp, covert, operation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16555</th>\n",
       "      <td>37122323</td>\n",
       "      <td>/m/0n5236t</td>\n",
       "      <td>Decoded</td>\n",
       "      <td>Jay-Z</td>\n",
       "      <td>2010-11-16</td>\n",
       "      <td>{\"/m/0xdf\": \"Autobiography\"}</td>\n",
       "      <td>The book follows very rough chronological ord...</td>\n",
       "      <td>[book, follow, rough, chronological, order, sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16557</th>\n",
       "      <td>37159503</td>\n",
       "      <td>/m/073nkd</td>\n",
       "      <td>Poor Folk</td>\n",
       "      <td>Fyodor Dostoyevsky</td>\n",
       "      <td>1846</td>\n",
       "      <td>{\"/m/02ql9\": \"Epistolary novel\", \"/m/014dfn\": ...</td>\n",
       "      <td>Makar Devushkin and Varvara Dobroselova are s...</td>\n",
       "      <td>[makar, devushkin, varvara, dobroselova, secon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9292 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       wikipedia    freebase                            title  \\\n",
       "0            843     /m/0k36               A Clockwork Orange   \n",
       "1            986     /m/0ldx                       The Plague   \n",
       "4           2152     /m/0x5g   All Quiet on the Western Front   \n",
       "5           2890    /m/011zx             A Wizard of Earthsea   \n",
       "7           4081    /m/01b4w  Blade Runner 3: Replicant Night   \n",
       "...          ...         ...                              ...   \n",
       "16548   36372465  /m/02vqwsp                   The Third Lynx   \n",
       "16550   36534061   /m/072y44                   Remote Control   \n",
       "16554   37054020  /m/04f1nbs                Transfer of Power   \n",
       "16555   37122323  /m/0n5236t                          Decoded   \n",
       "16557   37159503   /m/073nkd                        Poor Folk   \n",
       "\n",
       "                     author publicationdate  \\\n",
       "0           Anthony Burgess            1962   \n",
       "1              Albert Camus            1947   \n",
       "4      Erich Maria Remarque      1929-01-29   \n",
       "5         Ursula K. Le Guin            1968   \n",
       "7               K. W. Jeter      1996-10-01   \n",
       "...                     ...             ...   \n",
       "16548          Timothy Zahn            2007   \n",
       "16550            Andy McNab            1997   \n",
       "16554           Vince Flynn      2000-06-01   \n",
       "16555                 Jay-Z      2010-11-16   \n",
       "16557    Fyodor Dostoyevsky            1846   \n",
       "\n",
       "                                                  genres  \\\n",
       "0      {\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...   \n",
       "1      {\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...   \n",
       "4      {\"/m/098tmk\": \"War novel\", \"/m/016lj8\": \"Roman...   \n",
       "5      {\"/m/0dwly\": \"Children's literature\", \"/m/01hm...   \n",
       "7      {\"/m/06n90\": \"Science Fiction\", \"/m/014dfn\": \"...   \n",
       "...                                                  ...   \n",
       "16548                    {\"/m/06n90\": \"Science Fiction\"}   \n",
       "16550  {\"/m/01jfsb\": \"Thriller\", \"/m/02xlf\": \"Fiction...   \n",
       "16554   {\"/m/01jfsb\": \"Thriller\", \"/m/02xlf\": \"Fiction\"}   \n",
       "16555                       {\"/m/0xdf\": \"Autobiography\"}   \n",
       "16557  {\"/m/02ql9\": \"Epistolary novel\", \"/m/014dfn\": ...   \n",
       "\n",
       "                                                 summary  \\\n",
       "0       Alex, a teenager living in near-future Englan...   \n",
       "1       The text of The Plague is divided into five p...   \n",
       "4       The book tells the story of Paul Bäumer, a Ge...   \n",
       "5       Ged is a young boy on Gont, one of the larger...   \n",
       "7       Living on Mars, Deckard is acting as a consul...   \n",
       "...                                                  ...   \n",
       "16548   The story starts with former government agent...   \n",
       "16550   The series follows the character of Nick Ston...   \n",
       "16554   The reader first meets Rapp while he is doing...   \n",
       "16555   The book follows very rough chronological ord...   \n",
       "16557   Makar Devushkin and Varvara Dobroselova are s...   \n",
       "\n",
       "                                                prepared  \n",
       "0      [alex, teenager, living, england, lead, gang, ...  \n",
       "1      [text, plague, divide, five, part, town, oran,...  \n",
       "4      [book, tell, story, paul, bäumer, german, sold...  \n",
       "5      [ged, young, boy, go, one, large, island, nort...  \n",
       "7      [live, mar, deckard, act, consultant, movie, c...  \n",
       "...                                                  ...  \n",
       "16548  [story, start, former, government, agent, fran...  \n",
       "16550  [series, follow, character, nick, stone, man, ...  \n",
       "16554  [reader, first, meet, rapp, covert, operation,...  \n",
       "16555  [book, follow, rough, chronological, order, sw...  \n",
       "16557  [makar, devushkin, varvara, dobroselova, secon...  \n",
       "\n",
       "[9292 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8da10-7e78-491f-9f1d-0d44278181e6",
   "metadata": {},
   "source": [
    "### The algorithm\n",
    "\n",
    "Now for the machine learning algorithm. It's called Latent Dirichlet Allocation (LDA). It's a statistical model that is able to find words that often go together within a sample and then combine that with all of the samples to form groups where certain terms are just used very often.\n",
    "\n",
    "The downside of this unsupervised technique is that it won't tell you what the categories that it found are. Subject matter expertise is required to be able to discern that. Additionally, the model doesn't know how many different topics there are. It's a number you need to give it in advance. It requires a bit of trial and error.\n",
    "\n",
    "The LDA model is part of the gensim package, so we need to load that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b24b4ff3-e50c-446f-ac8b-43fbe75e6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212bc60f-b0e9-4509-9489-726cc0bb804b",
   "metadata": {},
   "source": [
    "Then we need to take the prepared text that we cleaned up earlier and create a dictionary out of it. This dictionary will contain all of the unique words it found in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7468223c-1a1d-4398-8cf5-253cac8391bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = gensim.corpora.Dictionary(list(books['prepared']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc9fe5-6097-4857-85dc-37578415c35d",
   "metadata": {},
   "source": [
    "We can check how many unique words there now are in this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faced100-ffa0-4181-9e0a-555d5de3373f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76292"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131ead7-00d9-4b1f-b78e-5e441d05c0e0",
   "metadata": {},
   "source": [
    "One more step we will want to do is to filter out the words that are extremely common and words that hardly appear at all. Removing the common words is similar to the step we had before where we removed the stop words, but this one will be over the entire corpus and will catch a few more that we missed before.\n",
    "\n",
    "The words that hardly appear should go too because if a word just appears a few times in the entire corpus, then there isn't much value in using this word to define a group of words that fit together. LDA will try to create groups based on words that are often found together after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc2e00c4-38df-47cb-9367-8037b9cde7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c8a391-f98b-4b46-bd17-3fb29c2fe0ef",
   "metadata": {},
   "source": [
    "Now we need to do a rather weird thing. We need to look at the first (or any really!) element of the `common_dictionary` we just filtered. This will force Python to load the dictionary in memory. If you don't do this, `id2token` will fail when training the model. This `id2token` function translates word IDs from the dictionary back into actual words which will make things easier for us to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "116f66ac-e245-4b71-b24b-c26e3fa79256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accidentally'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_dictionary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10162a37-46a1-4f16-b47e-d8daf79aa309",
   "metadata": {},
   "source": [
    "Now we have a dictionary that has been filtered, we can use it to transform our corpus of book summaries into something the LDA algorithm can understand. We need to transform the book summaries (documents) into a bag of words using the `doc2bow` function. A bag of words is just a list of words and the number of times that each word appeared in the document. Instead of using a word, it will instead use the word ID it gets from the dictionary since this is much more memory efficient and faster too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5278ddc1-489f-4dbe-890e-5e14a697f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus = [common_dictionary.doc2bow(text) for text in books['prepared']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f839e8-c183-4d10-85b3-3b29fbb0beb6",
   "metadata": {},
   "source": [
    "At this point, we will want to enable logging. This will let us see what the LDA model is doing while it is training. Otherwise, it will be too much of a black box. We will instruct Python to keep a log in \"lda_model.log\" and save everything from debug messages and upwards. Upwards in this context means information messages, warning messages, and error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24aed76f-0166-466b-8fd8-744c9adbadd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', filemode='w', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d3f4a-8aaa-491d-b078-98ad4a639202",
   "metadata": {},
   "source": [
    "Finally, everything is set up to do the training. Now we need to choose our model parameters! There are a lot of things you can tell the LDA model on how it should train. The most important one to choose is the number of topics.\n",
    "\n",
    "We need to tell the LDA model in advance how many groups we want it to create. Here we just say 10. It's not too high and it's not too low. There's really not much more rationale than that.\n",
    "\n",
    "We also set `eval_every` to `1` so that it will write something to the log for each pass.\n",
    "\n",
    "The number of `passes` tell the algorithm how often it should repeat the training. Each time it does so, it will be better able to find connections between words and form groups out of them. That is because the LDA model is trying to minimize \"word distances\" in a group while maximizing those for words in other groups. It's slowly shuffling this all around while making those losses smaller and smaller. We'll be able to see in the log how well it's doing. We'll just use 20 passes for now and see what we get.\n",
    "\n",
    "Then there is also `chunksize` which tells the algorithm how many books to process at the same time. Setting this high will let it find more correlations quicker but at the expense of using more computational power.\n",
    "\n",
    "Finally, the `random_state` just makes it so that every time you run this training with the above parameters, you will always get the same answer. It fixes that seed for the random number generator, which means that it will always generate the same \"random\" sequence of numbers used by the model. It makes everything reproducible. We pick the number `42` because why not! It only matters that it's fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2677b15e-319b-47e7-8d3a-00ef0399310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=common_corpus,\n",
    "    num_topics=10,\n",
    "    id2word=common_dictionary.id2token,\n",
    "    eval_every=1,\n",
    "    passes=20,\n",
    "    chunksize=3000,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eba169-255d-4658-bfee-e86d525bbaa4",
   "metadata": {},
   "source": [
    "Ok, our model is done training now. Let's open the file \"lda_model.log\" and look for the lines that say\n",
    "```\n",
    "DEBUG : 1453/3000 documents converged within 50 iterations\n",
    "```\n",
    "This number needs to be as high as possible. Ideally we want all documents to be converged but we will never get there because the groupings will be imperfect as these book summaries will be talking about a lot of different things and sometimes use common language.\n",
    "\n",
    "Once the model has been trained, we might want to save it to a file. That way when we reopen our Jupyter session, we don't need to retrain, but can just load it. You'll also be able to open it in a different notebook that's just focused on using the model if you want or share it with other researchers.\n",
    "\n",
    "Before we used something similar with pickle, but when sharing models, it's better to use a more standard data format since the pickle format may depend on the version of Python or even what computer you are using. Using standard data formats is a best practice in data management that increases interoperability and reproducibility in research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb106aa3-dee5-472c-9dc2-b44968c83268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "lda.save('bookmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "470b1d84-2a30-4e4e-ba7e-c927633f750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "lda = gensim.models.ldamodel.LdaModel.load('bookmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a814e-78d5-43bc-9da2-ef6efb509bea",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Time to look at the results. There's a number of things we can look at. For the ten topics we told the LDA to find, we can print the words it used and how strongly they indicated being part of a particular book genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86afbcb6-d1fc-49d1-8751-2ca8083d6c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.011*\"murder\" + 0.009*\"kill\" + 0.009*\"go\" + 0.008*\"police\" + 0.008*\"tell\" + 0.007*\"get\" + 0.006*\"case\" + 0.005*\"call\" + 0.005*\"day\" + 0.005*\"death\"')\n",
      "(1, '0.009*\"kill\" + 0.008*\"king\" + 0.007*\"return\" + 0.005*\"help\" + 0.005*\"use\" + 0.005*\"attack\" + 0.005*\"tell\" + 0.005*\"world\" + 0.005*\"back\" + 0.005*\"power\"')\n",
      "(2, '0.011*\"go\" + 0.008*\"get\" + 0.008*\"tell\" + 0.007*\"make\" + 0.006*\"leave\" + 0.006*\"see\" + 0.006*\"come\" + 0.006*\"island\" + 0.005*\"back\" + 0.005*\"say\"')\n",
      "(3, '0.017*\"novel\" + 0.016*\"book\" + 0.012*\"story\" + 0.008*\"character\" + 0.007*\"life\" + 0.006*\"world\" + 0.005*\"also\" + 0.005*\"first\" + 0.005*\"chapter\" + 0.005*\"include\"')\n",
      "(4, '0.019*\"john\" + 0.012*\"four\" + 0.012*\"go\" + 0.011*\"thomas\" + 0.011*\"henry\" + 0.011*\"sam\" + 0.008*\"luke\" + 0.008*\"tell\" + 0.007*\"sarah\" + 0.007*\"time\"')\n",
      "(5, '0.012*\"ship\" + 0.011*\"human\" + 0.010*\"earth\" + 0.009*\"planet\" + 0.007*\"time\" + 0.006*\"new\" + 0.006*\"world\" + 0.005*\"use\" + 0.005*\"year\" + 0.005*\"destroy\"')\n",
      "(6, '0.011*\"family\" + 0.010*\"father\" + 0.010*\"mother\" + 0.008*\"love\" + 0.008*\"life\" + 0.008*\"become\" + 0.008*\"child\" + 0.007*\"leave\" + 0.006*\"friend\" + 0.006*\"year\"')\n",
      "(7, '0.011*\"richard\" + 0.010*\"de\" + 0.009*\"max\" + 0.008*\"jake\" + 0.007*\"man\" + 0.007*\"hugh\" + 0.007*\"return\" + 0.007*\"leave\" + 0.007*\"tell\" + 0.006*\"day\"')\n",
      "(8, '0.011*\"war\" + 0.007*\"force\" + 0.007*\"kill\" + 0.006*\"army\" + 0.005*\"attempt\" + 0.005*\"attack\" + 0.005*\"state\" + 0.005*\"bond\" + 0.004*\"order\" + 0.004*\"use\"')\n",
      "(9, '0.013*\"harry\" + 0.012*\"get\" + 0.012*\"school\" + 0.010*\"jack\" + 0.009*\"go\" + 0.008*\"sydney\" + 0.008*\"ben\" + 0.006*\"work\" + 0.006*\"team\" + 0.006*\"name\"')\n"
     ]
    }
   ],
   "source": [
    "for t in lda.print_topics(num_words=10):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd1bc7-6c6f-4457-9466-36e9f9801b79",
   "metadata": {},
   "source": [
    "The first one seems very crime oriented with words like murder, kill, police, case, but you can see there are also some words in there that are not quite right. If you look at topic number 5, there definitely seems to be a Sci-Fi theme going on there with the words human, earth, planet, and world.\n",
    "\n",
    "There is however also some others where it's not quite clear how they are different genres. For example, topic 4 is mainly just names. Keep in mind that we are the ones looking for genres and that might not have been what the LDA algorithm found. It was just looking for anything to group these book summaries into ten topics.\n",
    "\n",
    "Another thing to look at is the top words per topic. It's similar to `print_topics` but now we are looking at coherence. Coherence in gensim is a metric that looks at the \"distance\" between words within a topic. A higher coherence score is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6d0088d-4ed4-485a-a51e-bee18ecc6984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.0110494755, 'family'),\n",
       "   (0.010247785, 'father'),\n",
       "   (0.009740395, 'mother'),\n",
       "   (0.008020293, 'love'),\n",
       "   (0.007960088, 'life'),\n",
       "   (0.007780727, 'become'),\n",
       "   (0.007565873, 'child'),\n",
       "   (0.007225185, 'leave'),\n",
       "   (0.006368225, 'friend'),\n",
       "   (0.0059221974, 'year'),\n",
       "   (0.005589299, 'go'),\n",
       "   (0.0055839773, 'young'),\n",
       "   (0.005487799, 'return'),\n",
       "   (0.0051857894, 'live'),\n",
       "   (0.0051689884, 'home'),\n",
       "   (0.0051486026, 'new'),\n",
       "   (0.004815906, 'meet'),\n",
       "   (0.004654479, 'daughter'),\n",
       "   (0.0044921557, 'marry'),\n",
       "   (0.004474495, 'make')],\n",
       "  -0.852357147559029),\n",
       " ([(0.009047911, 'kill'),\n",
       "   (0.0076246485, 'king'),\n",
       "   (0.006640664, 'return'),\n",
       "   (0.0054942826, 'help'),\n",
       "   (0.0052796644, 'use'),\n",
       "   (0.005254651, 'attack'),\n",
       "   (0.0051531107, 'tell'),\n",
       "   (0.0051329145, 'world'),\n",
       "   (0.004670613, 'back'),\n",
       "   (0.004652856, 'power'),\n",
       "   (0.0043452256, 'make'),\n",
       "   (0.004283357, 'leave'),\n",
       "   (0.004097437, 'go'),\n",
       "   (0.0039577885, 'magic'),\n",
       "   (0.0037244798, 'become'),\n",
       "   (0.0037072925, 'give'),\n",
       "   (0.0036736762, 'also'),\n",
       "   (0.0036432738, 'order'),\n",
       "   (0.0035776694, 'see'),\n",
       "   (0.0035167304, 'force')],\n",
       "  -0.8798599979278723),\n",
       " ([(0.011066067, 'murder'),\n",
       "   (0.009275636, 'kill'),\n",
       "   (0.00888664, 'go'),\n",
       "   (0.00845129, 'police'),\n",
       "   (0.00800952, 'tell'),\n",
       "   (0.00742133, 'get'),\n",
       "   (0.005969997, 'case'),\n",
       "   (0.005172954, 'call'),\n",
       "   (0.0051046726, 'day'),\n",
       "   (0.0050141024, 'death'),\n",
       "   (0.0050025163, 'later'),\n",
       "   (0.0048860894, 'say'),\n",
       "   (0.004859923, 'leave'),\n",
       "   (0.00480853, 'time'),\n",
       "   (0.00438591, 'back'),\n",
       "   (0.0043737623, 'discover'),\n",
       "   (0.004306422, 'man'),\n",
       "   (0.0042611184, 'try'),\n",
       "   (0.004226807, 'give'),\n",
       "   (0.0041537397, 'body')],\n",
       "  -0.8994934725642094),\n",
       " ([(0.010877573, 'go'),\n",
       "   (0.008306963, 'get'),\n",
       "   (0.0077550416, 'tell'),\n",
       "   (0.0066984645, 'make'),\n",
       "   (0.0059883306, 'leave'),\n",
       "   (0.0058463197, 'see'),\n",
       "   (0.0058085267, 'come'),\n",
       "   (0.00555334, 'island'),\n",
       "   (0.005428566, 'back'),\n",
       "   (0.0053745257, 'say'),\n",
       "   (0.005361099, 'boy'),\n",
       "   (0.0051668733, 'man'),\n",
       "   (0.0048363074, 'call'),\n",
       "   (0.004833694, 'cat'),\n",
       "   (0.004583741, 'two'),\n",
       "   (0.0045204535, 'day'),\n",
       "   (0.0043924847, 'little'),\n",
       "   (0.00428675, 'old'),\n",
       "   (0.004215559, 'book'),\n",
       "   (0.0041349875, 'way')],\n",
       "  -1.1089524416940162),\n",
       " ([(0.016524425, 'novel'),\n",
       "   (0.01635563, 'book'),\n",
       "   (0.011665621, 'story'),\n",
       "   (0.0078525655, 'character'),\n",
       "   (0.007079506, 'life'),\n",
       "   (0.0058026775, 'world'),\n",
       "   (0.005374613, 'also'),\n",
       "   (0.004859394, 'first'),\n",
       "   (0.0046915784, 'chapter'),\n",
       "   (0.0046431622, 'include'),\n",
       "   (0.004371545, 'end'),\n",
       "   (0.004213073, 'describe'),\n",
       "   (0.0041669966, 'time'),\n",
       "   (0.0041174893, 'write'),\n",
       "   (0.0040176734, 'people'),\n",
       "   (0.0039546695, 'become'),\n",
       "   (0.0038098574, 'part'),\n",
       "   (0.0037996897, 'event'),\n",
       "   (0.003589654, 'work'),\n",
       "   (0.0035832277, 'set')],\n",
       "  -1.1586394257033297),\n",
       " ([(0.011816035, 'ship'),\n",
       "   (0.0106709255, 'human'),\n",
       "   (0.009998546, 'earth'),\n",
       "   (0.008778745, 'planet'),\n",
       "   (0.006789669, 'time'),\n",
       "   (0.0057856753, 'new'),\n",
       "   (0.005676125, 'world'),\n",
       "   (0.0052419775, 'use'),\n",
       "   (0.0047859857, 'year'),\n",
       "   (0.0047349636, 'destroy'),\n",
       "   (0.0046564755, 'system'),\n",
       "   (0.0044906987, 'alien'),\n",
       "   (0.0044494793, 'crew'),\n",
       "   (0.0042965407, 'two'),\n",
       "   (0.0040266784, 'space'),\n",
       "   (0.0038974553, 'discover'),\n",
       "   (0.0038509928, 'become'),\n",
       "   (0.0036873547, 'race'),\n",
       "   (0.0035617596, 'force'),\n",
       "   (0.0034394104, 'group')],\n",
       "  -1.2880320281603785),\n",
       " ([(0.019350901, 'john'),\n",
       "   (0.012343307, 'four'),\n",
       "   (0.011951407, 'go'),\n",
       "   (0.011256119, 'thomas'),\n",
       "   (0.010943421, 'henry'),\n",
       "   (0.010530382, 'sam'),\n",
       "   (0.007921846, 'luke'),\n",
       "   (0.0075592403, 'tell'),\n",
       "   (0.0072635054, 'sarah'),\n",
       "   (0.0066445973, 'time'),\n",
       "   (0.0066089514, 'six'),\n",
       "   (0.006549933, 'mark'),\n",
       "   (0.006211759, 'back'),\n",
       "   (0.0060505266, 'say'),\n",
       "   (0.005687484, 'escape'),\n",
       "   (0.005570402, 'see'),\n",
       "   (0.0054911813, 'get'),\n",
       "   (0.0054528024, 'number'),\n",
       "   (0.0053912117, 'also'),\n",
       "   (0.005212229, 'try')],\n",
       "  -1.6273870567537805),\n",
       " ([(0.010955997, 'war'),\n",
       "   (0.007185829, 'force'),\n",
       "   (0.006688412, 'kill'),\n",
       "   (0.0055585653, 'army'),\n",
       "   (0.005357544, 'attempt'),\n",
       "   (0.005083893, 'attack'),\n",
       "   (0.005018706, 'state'),\n",
       "   (0.004784857, 'bond'),\n",
       "   (0.0043996433, 'order'),\n",
       "   (0.004351693, 'use'),\n",
       "   (0.0042293402, 'officer'),\n",
       "   (0.004001528, 'plan'),\n",
       "   (0.0038744204, 'agent'),\n",
       "   (0.00381393, 'lead'),\n",
       "   (0.0037828526, 'british'),\n",
       "   (0.0036342563, 'united'),\n",
       "   (0.003631425, 'general'),\n",
       "   (0.0034998427, 'government'),\n",
       "   (0.0034448202, 'american'),\n",
       "   (0.0034325018, 'send')],\n",
       "  -1.6469919357467118),\n",
       " ([(0.012514268, 'harry'),\n",
       "   (0.011745077, 'get'),\n",
       "   (0.01160046, 'school'),\n",
       "   (0.010107191, 'jack'),\n",
       "   (0.009295834, 'go'),\n",
       "   (0.0084613105, 'sydney'),\n",
       "   (0.0075095803, 'ben'),\n",
       "   (0.006206516, 'work'),\n",
       "   (0.005804462, 'team'),\n",
       "   (0.0056178784, 'name'),\n",
       "   (0.0054714642, 'see'),\n",
       "   (0.0052048955, 'back'),\n",
       "   (0.0050857826, 'call'),\n",
       "   (0.0050535523, 'meet'),\n",
       "   (0.005051035, 'time'),\n",
       "   (0.0049523804, 'make'),\n",
       "   (0.0049423566, 'rise'),\n",
       "   (0.004920191, 'adrian'),\n",
       "   (0.004907922, 'tell'),\n",
       "   (0.004893738, 'book')],\n",
       "  -1.6768336028652775),\n",
       " ([(0.011130674, 'richard'),\n",
       "   (0.009534553, 'de'),\n",
       "   (0.008858717, 'max'),\n",
       "   (0.0075008064, 'jake'),\n",
       "   (0.007250416, 'man'),\n",
       "   (0.007025279, 'hugh'),\n",
       "   (0.006880192, 'return'),\n",
       "   (0.0065772273, 'leave'),\n",
       "   (0.0065650116, 'tell'),\n",
       "   (0.0063807275, 'day'),\n",
       "   (0.0061049056, 'father'),\n",
       "   (0.005954069, 'abbey'),\n",
       "   (0.005862775, 'brother'),\n",
       "   (0.0057803867, 'rachel'),\n",
       "   (0.0053491513, 'two'),\n",
       "   (0.0052772104, 'back'),\n",
       "   (0.0052246293, 'sir'),\n",
       "   (0.0051900796, 'see'),\n",
       "   (0.0050810473, 'come'),\n",
       "   (0.0045155785, 'go')],\n",
       "  -1.9037630489461803)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.top_topics(common_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1e291-1f2f-4f2b-8431-cfca3eb292d8",
   "metadata": {},
   "source": [
    "So for our first book with index 0, \"Clockwork Orange\", we can see the groups that LDA thinks it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee8abd8c-2e70-43e1-a589-4537b8952a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Clockwork Orange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.44291747), (3, 0.26078534), (6, 0.21150394), (8, 0.07269716)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(books.iloc[0].title)\n",
    "lda[common_corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab61522-12ff-41a0-ab32-f904a6019356",
   "metadata": {},
   "source": [
    "The numbers here show the group number followed by the likelihood of this book belonging to that group. Here the LDA thinks that \"A Clockwork Orange\" belongs to group 0 with 44.3% confidence, which is not a whole lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bcef3f-00e8-4070-b212-e735ca084fb1",
   "metadata": {},
   "source": [
    "Another thing we can do is look at books that most definitely belong to a group. We'll have to do some Python magic. First we create a function that takes the above output and transforms it into something that is easier to work with by adding zeroes for all the groups that are not part of the output. That is to say, for \"Clockwork Orange\" we have the groups 0, 3, 6, and 8, so we'll create new entries for the missing 1, 2, 4, 5, 7, and 9 and set those to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c58f67bc-6765-4988-9ac6-4c079fa805c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def densify(sparse, num_topics=10):\n",
    "    full = [0]*num_topics\n",
    "    for s in sparse:\n",
    "        full[s[0]] = s[1]\n",
    "    return full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def450eb-2f69-44fc-84a9-9011aec89f15",
   "metadata": {},
   "source": [
    "Using that function, we can now convert the entire output of the LDA model for every book into a 9292 (number of books) by 10 (number of groups) matrix and load that into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "421f0f69-35f5-4158-8a68-99499e226b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicmatrix=pd.DataFrame([densify(lda[c]) for c in common_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f29aae-15be-43ef-a484-b88123a79d63",
   "metadata": {},
   "source": [
    "That in turn we can join to the original `books` DataFrame. We just need to reset the index on the `books` DataFrame because earlier we threw away a whole bunch of books for having missing values. Resetting the index will make sure the index runs from 0 to 9291 which is then the same as our `topicmatrix` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26ff1529-0f41-468f-8f36-441327f3c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = books.reset_index().join(topicmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da347212-fd23-4b1f-82ce-cfe1bb8fd137",
   "metadata": {},
   "source": [
    "Now we have that, we can use Pandas to find us the books in group 2 with a confidence higher than 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7bbd474-68f0-43a7-93d6-4e1720221050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3695                           Punk Farm\n",
       "5340     The Moomins and the Great Flood\n",
       "5699    The Story of A Fierce Bad Rabbit\n",
       "5943            Five Go Off In A Caravan\n",
       "6013         Curious George Flies a Kite\n",
       "6437                          Fox's Feud\n",
       "8034                   The Missing Piece\n",
       "8095                      Curious George\n",
       "8359                 Battle for the Park\n",
       "8737                              Bedlam\n",
       "9003                     The Sly Old Cat\n",
       "9035         Curious George Gets a Medal\n",
       "9141                       The Gathering\n",
       "9150                         Troll Blood\n",
       "9210                      Little Red Cap\n",
       "9211                When the Moon Forgot\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined[joined[2] > 0.95][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f6961a-82bf-41ba-80f7-ce4d4cb805c6",
   "metadata": {},
   "source": [
    "Now it's up to us again to think about what these books might have in common. Judging from the titles, most of these sounds like children's books!\n",
    "\n",
    "What do we have for group 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0193a9f-8806-4d24-bce2-a7e02c8bf28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855                             The Dragon Reborn\n",
       "1123                     The Wishsong of Shannara\n",
       "1546                         To Green Angel Tower\n",
       "2029                          The Source of Magic\n",
       "3496                                 Resurrection\n",
       "3751                          Dawn of the Dragons\n",
       "3777                                   Stronghold\n",
       "3804                       The Kingdoms of Terror\n",
       "3805                                 Castle Death\n",
       "3808                       The Dungeons of Torgar\n",
       "3896                        The Hunger of Sejanoz\n",
       "3909                                    Vampirium\n",
       "3910                   The Fall of Blood Mountain\n",
       "3963                         The Hour of the Gate\n",
       "4597                           Chosen of the Gods\n",
       "4860                         The Bone Doll's Twin\n",
       "4879                                Deryni Rising\n",
       "4966                               Rise of a Hero\n",
       "4979                                   Shadowplay\n",
       "5063                      Dark Wraith of Shannara\n",
       "5997                      Tanis, the Shadow Years\n",
       "6095                            The Eternal Flame\n",
       "6485           The Twelve Kingdoms: Sea of Shadow\n",
       "6499                          Master of the Books\n",
       "7045                    The Seven Songs of Merlin\n",
       "7403                            The Riven Kingdom\n",
       "7658                    Beyond the Nightmare Gate\n",
       "7672                           War of the Wizards\n",
       "7844                            The Kiss of Death\n",
       "8069                           Moon of the Spider\n",
       "8283           The Twelve Kingdoms: Skies of Dawn\n",
       "8857                               The White Hart\n",
       "8920    World of Warcraft: Beyond the Dark Portal\n",
       "9069                                Lord of Souls\n",
       "9095                         Rage of a Demon King\n",
       "9100                                    Whizzard!\n",
       "9119                     Talon of the Silver Hawk\n",
       "9120                                King of Foxes\n",
       "9123                               Exile's Return\n",
       "9178                      The Tower at Stony Wood\n",
       "9192                                   Storm Born\n",
       "9209                                Dragon's Oath\n",
       "9221                                     Awakened\n",
       "9223                    Infinity Blade: Awakening\n",
       "9230                                  Neverwinter\n",
       "9236                                        Darke\n",
       "9260                        Quest for Lost Heroes\n",
       "9264        Supplement to the Journey to the West\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined[joined[1] > 0.95][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adf7ce-3817-42ec-9d89-2c2e2c02c188",
   "metadata": {},
   "source": [
    "Hmm, fantasy maybe?\n",
    "\n",
    "It's not always very clear though. For example, what would be the theme for group 9?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b755f4d9-f0ab-4fc1-a127-23d7d25ab3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8782    Ishmael and the Return of the Dugongs\n",
       "9017                               Bloodlines\n",
       "9103                       Soccer Comes First\n",
       "9105                                     Goal\n",
       "9202                        Succubus Revealed\n",
       "9237                The Luck of Ginger Coffey\n",
       "9268                  Big Nate: Strikes Again\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined[joined[9] > 0.95][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c2cbe-0f17-454a-aa77-75e100e52f73",
   "metadata": {},
   "source": [
    "There are not a lot of books here, so even the LDA isn't too sure about what groups these books together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d864ef9-3c9f-45fc-b061-3f683b41910b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "That's the trouble with unsupervised learning. The algorithm will find things that belong together but it doesn't tell you why. It can only tell you which words it used to group these books but as far as the model is concerned, these words are just indexes in a dictionary. All it sees are numbers and the words have no meaning.\n",
    "\n",
    "At this point, you would start playing with the parameters. For example, maybe we chose too many topics or too few. Other parameters to play with are the number of training passes and the chunk size. In the [manual for the LDA model](https://radimrehurek.com/gensim/models/ldamodel.html) there are also a few other parameters you can use that we didn't set here. It will be a balance between your computational resources and accuracy. You will need to keep a close eye on the log file that the LDA model produces to see if your parameters have the desired effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162304c-8ca3-4a84-b73c-6d7ebff96b28",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "- https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c1a7d-4fac-4124-9b2f-dac0575c6624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
