{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e970877b-11f7-49ff-ad71-412f8b298055",
   "metadata": {},
   "source": [
    "# Traitement non dirigé du langage naturel\n",
    "\n",
    "Le traitement non dirigé du langage naturel consiste à traiter le langage naturel sans spécifier aucune étiquette. C’est l’algorithme d’apprentissage-machine qui identifiera lui-même les groupes. C’est une technique très utile si vous ignorez au départ quelles sont les thématiques communes d’un ensemble de documents. Cela vous permet d’identifier des tendances que vous ne pouvez discerner par vous-même, en raison par exemple du vaste nombre de documents que vous souhaitez étudier. \n",
    "\n",
    "Nous verrons plus avant dans le présent carnet commet rendre des textes plus faciles à lire et à parcourir pour un ordinateur, et par la suite comment appliquer cette technique à la classification de livres en fonction de leurs résumés. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2635976-d682-48e0-9355-d9102881c99b",
   "metadata": {},
   "source": [
    "## Les objectifs d’apprentissage\n",
    "\n",
    "Durée moyenne d’exécution : 60 minutes\n",
    "\n",
    "À la fin de ce tutoriel, vous devriez être en mesure :\n",
    "* D’épurer vos données et d’expliquer en quoi cela est important pour l’apprentissage-machine.\n",
    "* De ramener les mots à leur signification première pour créer des groupes.\n",
    "* D’interpréter ces groupes en utilisant différentes façons de voir les données.  \n",
    "* De décrire ce qu’est l’apprentissage non dirigé et comment nous l’utilisons. \n",
    "* De décrire ce qu’est le traitement automatique du langage et comment l’utiliser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf7e12-e9f0-4b35-8127-f89e21284a26",
   "metadata": {},
   "source": [
    "## Ce dont vous aurez besoin pour compléter le tutoriel\n",
    "\n",
    "* Consultez le [document d'introduction](https://uottawa-it-research-teaching.github.io/machinelearning/) pour connaître les exigences générales et le fonctionnement des carnets Jupyter.\n",
    "* Nous aurons aussi besoin de Pandas pour faciliter la gestion des données. C’est un outil Python très puissant, capable de lire les fichiers CSV et Excel. Il offre aussi d’excellentes capacités de manipulation de données, ce qui est très utile pour l’épuration des données.\n",
    "* Nous utiliserons NLTK comme trousse d’apprentissage-machine. NLTK est l’acronyme de Natural Language Tool Kit ou en français, Trousse d’outils de langage naturel.\n",
    "* Des fichiers de données qui sont intégrées au présent carnet dans le dossier « données »."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36945106-a028-4e93-80e0-7a076ff960b4",
   "metadata": {},
   "source": [
    "## Les meilleures pratiques de gestion des données de recherche (GDR)\n",
    "\n",
    "Une bonne manipulation des données destinées à l’apprentissage-machine commence par une gestion efficace des données de recherche (GDR). La qualité de vos données de base aura une incidence sur vos éventuels résultats. Au même titre, la reproductibilité de vos résultats dépendra de vos données de base et de la façon dont vous organiserez vos données pour permettre à d’autres personnes (et aux machines !) de comprendre ces données et de pouvoir les réutiliser.\n",
    "\n",
    "Nous devrons aussi constamment recourir aux meilleures pratiques de gestion des données de recherche, pratiques recommandées par l'[Alliance de recherche numérique du Canada](https://zenodo.org/records/4012530). Nous vous avons encouragé dans le premier tutoriel à vous conformer à ces deux meilleures pratiques de gestion des données de recherche :\n",
    "\n",
    "ENREGISTREZ VOS DONNÉES BRUTES DANS LEUR FORMAT ORIGINAL\n",
    "* N’écrasez pas vos données originales avec une version épurée.\n",
    "* Protégez vos données originales en les verrouillant ou en les rendant inaltérables (lecture seule). \n",
    "* Revenez à ces données originales en cas de problème (ce qui se produit souvent).\n",
    "\n",
    "SAUVEGARDEZ VOS DONNÉES\n",
    "* Utilisez la règle du 3-2-1: sauvegardez trois copies de vos données, deux sur deux différents supports de stockage et la dernière hors site. Le stockage hors site pourra se faire sur OneDrive ou Google drive, ou tout autre site fourni par votre établissement.  \n",
    "* Nous utilisons Open Data qui ne comporte pas de données identifiables ou de données devant faire l’objet de restriction ou protection particulière.  Mais si vos données comportent des renseignements confidentiels, assurez-vous de prendre les mesures nécessaires pour en limiter l’accès ou pour chiffrer vos données.\n",
    "\n",
    "Il y a quelques autres meilleures pratiques de gestion des données de recherche qui vous aideront à gérer votre projet. Nous les mettrons en évidence au début de chaque tutoriel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3d7b4-9dbe-4c85-a586-ec9c1fcf8b4b",
   "metadata": {},
   "source": [
    "## Épuration des données aux fins de traitement du langage naturel\n",
    "\n",
    "Il y a plusieurs choses que vous devez faire pour épurer des données destinées au traitement automatique du langage naturel. Tout dépend de la tâche que vous souhaitez accomplir : \n",
    "* Éliminer les mots vides\n",
    "* Supprimer la ponctuation\n",
    "* Lemmatisation\n",
    "* Remplacement par des synonymes\n",
    "\n",
    "Les mots vides sont des mots si répandus qu’on les retrouve partout. Ce sont des mots comme « un », « le », « est », « ont », etc. Ils n’ont donc aucune valeur le temps venu de catégoriser des textes. Vous voudrez plutôt vous concentrer sur des mots qui sont plus ou moins spécifiques à des catégories données. À titre d’exemple, la langue anglaise est victimes de contractions comme « you'll » qui doivent être rétablies. \n",
    "\n",
    "La contribution de la ponctuation est aussi plutôt mince et nous pouvons donc l’éliminer. \n",
    "\n",
    "La lemmatisation est une technique de réduction de la conjugaison de verbes qui permet de les ramener à leur forme de base et de supprimer les pluriels. À titre d’exemple, les formes « acheter », « achètes » et « achetés » qui proviennent tous du même verbe sont néanmoins perçues différemment par un ordinateur. En remplaçant toutes ces conjugaisons par simplement « acheter », un ordinateur peut y voir une seule et même signification. \n",
    "\n",
    "Les synonymes sont des mots qui ont une même signification. Idéalement, vous voudriez que les mots « aide » et « assistance » soient comptés comme un seul mot. Mais y arriver est extrêmement difficile car certains synonymes ne sont pas toujours identiques et peuvent avoir une signification quelque peu différente. Il n’est donc pas toujours souhaitable qu’ils soient comptés comme un seul mot. \n",
    "\n",
    "Les paquets de traitement automatique du langage naturel contiennent habituellement leur propre liste de mots vides. Mais ce ne sont pas toujours les mêmes. La définition même d’un mot vide est un peu ambiguë. Toutes associées à Python, les trousses NLTK, SpaCy, Gensim et Scikit-learn contiennent des listes différentes. Dans notre cas, nous utiliserons la trousse NLTK.\n",
    "\n",
    "Nous avons besoin en anglais du paquet « contractions » qui n’est pas systématiquement installé par défaut. Si vous n’avez pas ce paquet, référez-vous au premier carnet pour connaitre son mode d’installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dada4488-c64f-4c30-aed7-417fec71f2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jvanderk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jvanderk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import contractions\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d129e-e59c-4b8a-993c-8c49cbfab8bf",
   "metadata": {},
   "source": [
    "### Les mots vides\n",
    "\n",
    "Voici les dix premiers mots de la liste des mots vides en anglais et en français :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ad6b63-6740-4341-988f-24d6e1f49793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b16130e-737f-4b8f-9095-3d0177a00dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b01fb-ab19-43aa-9142-3e12756a4361",
   "metadata": {},
   "source": [
    "Vous remarquerez que tous ces mots sont en lettres minuscules. Veiller à ce que tous les mots soient en lettres minuscules est une bonne habitude car vous n’aurez pas à vous préoccuper du respect de la casse pour comparer des mots. Filtrons donc cette phrase pour en supprimer les mots vides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3d6cfe-af5e-4fb5-bf34-c89600737097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notice words lower case . good practise make words lower case , worry case sensitivity comparing words . let us filter sentence .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"You'll notice that all of the words are lower case too. It's good practise to make all words lower case, so you don't have to worry about case sensitivity when comparing words. Let's filter this sentence.\"\n",
    "\n",
    "# Make it all lower case\n",
    "text = text.lower()\n",
    "\n",
    "# Remove contactions\n",
    "text = contractions.fix(text)\n",
    "\n",
    "# Tokenize the text which splits the words and punctiation\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove the stopwords and make everything lower case\n",
    "filtered_text = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "# Show result in a readable way\n",
    "\" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602bc7e-4795-401b-b779-df41454bc540",
   "metadata": {},
   "source": [
    "### Supprimer la ponctuation\n",
    "\n",
    "Il est assez facile de le faire en utilisant la fonction isalpha qui ne conserve que les éléments qui contiennent des lettres. Chiffres et ponctuation disparaitront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6aea04f-aff9-4843-a74f-a09787e6d9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notice words lower case good practise make words lower case worry case sensitivity comparing words let us filter sentence'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token for token in filtered_text if token.isalpha()]\n",
    "\n",
    "# Show result in a readable way\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17ba98-10c0-4dd6-ac28-85e2735c734d",
   "metadata": {},
   "source": [
    "### La lemmatisation\n",
    "Malheureusement, la trousse NLTK n’offre la lemmatisation qu’en langue anglaise.  Il existe toutefois un outil pilote de lemmatisation en français qui peut remplacer celui de la trousse NLTK. Cet outil porte le nom de [LEFF Lematizer](https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer). Utilisez-le pour analyser des textes en français. Dans le cas présent, nous analyserons des livres en anglais et utiliserons par conséquent l’outil par défaut de la trousse NLTK.\n",
    "\n",
    "Le code ci-après génère le lemmatisateur puis analyse tous les mots pour en extraire le lemme. Nous utiliserons à nouveau la fonction `join` pour afficher les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ff4a3e-2b61-4af0-a4ff-e1734fa45720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notice word lower case good practise make word lower case worry case sensitivity comparing word let u filter sentence'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90cd0a-246e-46ca-b592-cf1f5425d3a0",
   "metadata": {},
   "source": [
    "Mais il se passe quelque chose d’étrange ! Le mot « comparing » est conservé alors qu’il aurait dû devenir « compare ».  Cela découle du fait que WordNet présume que tout élément est un nom, à moins d’indication contraire.  \n",
    "\n",
    "Voici ce que génère le lemmatiseur pour le terme « comparing » :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d232557-23eb-47f4-b6a8-752b820e95b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comparing'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('comparing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0d342-ca4c-4c4e-b27f-db2807cf96d6",
   "metadata": {},
   "source": [
    "NLTK l’interprète comme s’il s’agissait de « The Comparing » et laisse le mot tel quel.  Mais nous voulons qu’il soit marqué comme verbe afin que le lemmatiseur puisse le ramener (conjuguer) à sa forme de base.\n",
    "\n",
    "Si nous indiquons clairement que le mot dont nous souhaitons extraire le lemme est un verbe, tout ira bien. Nous pouvons le faire en spécifiant `pos='v'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f5ff15d-1dd0-4289-9161-f0cfac6a59e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'compare'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same code but now marked as verb\n",
    "lemmatizer.lemmatize('comparing', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582220b0-94e8-4a34-9892-0eeb132c4dc4",
   "metadata": {},
   "source": [
    "Heureusement, nous n’avons pas à marquer nous-mêmes les mots en fonction de leur classe, c’est-à-dire les désigner comme verbe, nom ou autre chose. Nous pouvons utiliser la fonction « pos_tag » qui fera le travail à notre place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed92b29e-0ca2-4f05-95a2-2b5776dc4738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('notice', 'NN'),\n",
       " ('word', 'NN'),\n",
       " ('lower', 'JJR'),\n",
       " ('case', 'NN'),\n",
       " ('good', 'JJ'),\n",
       " ('practise', 'NN'),\n",
       " ('make', 'VBP'),\n",
       " ('word', 'NN'),\n",
       " ('lower', 'JJR'),\n",
       " ('case', 'NN'),\n",
       " ('worry', 'NN'),\n",
       " ('case', 'NN'),\n",
       " ('sensitivity', 'NN'),\n",
       " ('comparing', 'VBG'),\n",
       " ('word', 'NN'),\n",
       " ('let', 'NN'),\n",
       " ('u', 'JJ'),\n",
       " ('filter', 'NN'),\n",
       " ('sentence', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91dd2fd-8e88-4b74-947f-93249b03253d",
   "metadata": {},
   "source": [
    "Elle classe tous les mots en utilisant un modèle pré-entrainé, inclus dans la trousse NLTK. On peut spécifier la langue, en sélectionnant par exemple `lang='fra'` pour le français.\n",
    "\n",
    "Les codes générés comme NN et VBG, signifient respectivement « Nom, singulier ou masse » et « Verbe, gérondif ou participe présent ». Cela dépend toutefois de l’ensemble de balises que vous utilisez. Celui de NLTK en anglais est [Penn Treebank tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) et c’est celui que nous utiliserons. \n",
    "\n",
    "Ces codes doivent être traduits pour faire le balisage sous WordNet. Nous pouvons créer une fonction pour cela. Pour une raison quelconque, cette fonction n’est pas intégrée à la trousse NTLK. Peut-être est-ce parce que les étiquettes varient selon les ensembles de balises. Mais nous pouvons tout simplement regarder la première lettre de l’étiquette et l’utiliser. De fait, si nous prenons les étiquettes de Penn Treebank, voici ce que nous obtenons :\n",
    "* VB Verbe, forme de base\n",
    "* VBD Verbe, passé\n",
    "* VBG Verbe, gérondif ou participe présent\n",
    "* ...\n",
    "Elles commencent toutes par « V » car ce sont tous des verbes. La classification est toutefois un peu plus détaillée. Nous pouvons donc procéder de la même façon avec les autres catégories, à l’instar du V, soit « N » pour noms, « J » pour adjectifs, etc. \n",
    "\n",
    "La fonction permettant de traduire d’un système de balisage à un autre est donc :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c29e881f-2c99-448c-b156-bdf737dacac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    elif tag.startswith('S'):\n",
    "        return nltk.corpus.wordnet.ADJ_SAT\n",
    "    else:\n",
    "        # If it's something else, then just use the default value for the lemmatizer\n",
    "        return nltk.corpus.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33880df6-0c1c-4371-bb95-9b880db61064",
   "metadata": {},
   "source": [
    "D’un point de vue technique, il est préférable de procéder au balisage avant de supprimer la ponctuation et d’exécuter le balisage en fonction de phrases car la ponctuation a une portée sémantique que le marqueur peut utiliser. Mais l’ordre que nous avons utilisé convenait davantage à des fins éducatives. Pour l’exemple concret un peu plus loin ci-dessous, nous suivrons le bon ordre. Mais voyons tout de même ce que nous avons obtenu avec les mots étiquetés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42afe0f2-083f-4098-a2a8-96a484614a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notice word low case good practise make word low case worry case sensitivity compare word let u filter sentence'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in nltk.pos_tag(tokens)]\n",
    "\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa587545-d18e-477a-bc29-34883f27951a",
   "metadata": {},
   "source": [
    "Presque chaque mot a maintenant été ramené à sa forme de base, à sa racine, même si cela sonne un peu comme un pirate!\n",
    "\n",
    "Ce n’est donc pas parfait. Vous remarquerez que nous avons maintenant « u » plutôt que « us ». C’est parce que « us » a été classifié à tort comme un adjectif (JJ). Il a peut-être été perçu comme la chose appartenant à « u » comme dans « us thing ». Mais dans tous les cas, cela demeure acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a071f43-a76a-4bba-b03f-7291089153c4",
   "metadata": {},
   "source": [
    "## De retour aux livres\n",
    "\n",
    "Nous utiliserons le jeu de données sur les livres de la CMU que nous avons utilisé préalablement dans les tutoriels sur les classificateurs à noyaux et bayésiens. Mais cette fois-ci, nous utiliserons un algorithme de classification non dirigée pour voir ce qu’il parvient à identifier. « Non dirigée » signifie que nous n’utiliserons aucune des étiquettes qui viennent avec le jeu de données. Nous laisserons plutôt notre algorithme d’apprentissage-machine se débrouiller seul. Il tentera de localiser des résumés de livres susceptibles d’être regroupés pour ensuite nous expliquer sa façon de penser.\n",
    "\n",
    "Par contre, l’apprentissage non dirigé étant incapable de comprendre la matière analysée, il vous reviendra de figurer quels sont les genres identifiés.\n",
    "\n",
    "### Lire les données\n",
    "Nous lirons les données comme nous l’avons fait dans le carnet sur les plus proches voisins. Nous utiliserons pour cela Pandas et JSON pour charger le jeu de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4888ad9-c8c5-4958-9766-c86ce1e36340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec742d-f6a1-49f3-9b87-7e5c1581df37",
   "metadata": {},
   "source": [
    "Nous lirons ensuite les résumés de livres contenus dans nos fichiers de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f04635c9-c24d-4af1-a4a7-da8d086ca049",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv('../data/booksummaries.txt', sep=\"\\t\", header=0, names=['wikipedia', 'freebase', 'title', 'author', 'publicationdate', 'genres', 'summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2183ea-0d01-44a9-8a67-018c13953757",
   "metadata": {},
   "source": [
    "Nous ignorerons toutes les lignes contenant des données manquantes car nous avons suffisamment de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ed2b997-25f2-4d20-9964-0dee2864018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3e326-44fe-4af4-adea-8a6d90b3f080",
   "metadata": {},
   "source": [
    "Puis nous créerons une fonction pour extraire les genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8f051a9-8929-448b-a628-b6fb87530df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre(row):\n",
    "    g = json.loads(row.genres)\n",
    "    return list(g.values())\n",
    "\n",
    "#genresperbook = books.apply(genre, axis=1)\n",
    "#books = books.assign(genres=genresperbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eec9de-bbae-4ab5-8a52-3d9178f9e797",
   "metadata": {},
   "source": [
    "Vous remarquerez que nous avons décommenté les genres de lecture contenus dans notre fiche de données Pandas, comparativement à la fonction identique dans le carnet sur les plus proches voisins, et cela, parce que nous n’avons pas besoin de données étiquetées. En effet, nous utilisons un algorithme non dirigé.\n",
    "\n",
    "### Préparation des données\n",
    "\n",
    "Allons-y ! Faisons les modifications requises pour n’avoir que des lettres minuscules et supprimer les contractions avant de segmenter le texte en unités lexicales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8986f717-aed1-4b05-a1ff-999f83318898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Clockwork Orange'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which book are we looking at?\n",
    "books.loc[0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d60d8a-5504-4f3b-9db1-4cdf0cb3316e",
   "metadata": {},
   "source": [
    "Allons-y ! Faisons les modifications requises pour n’avoir que des lettres minuscules et supprimer les contractions avant de segmenter le texte en unités lexicales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9892442e-c3f0-4d9b-896d-6ee9d68bc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it all lower case\n",
    "text = books.loc[0]['summary'].lower()\n",
    "\n",
    "# Remove contactions\n",
    "text = contractions.fix(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08276414-85c5-482b-9e0e-b37b58bffd20",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Procédons maintenant au balisage des mots avant de les charger dans le lemmatiseur pour obtenir la forme de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59527d0a-4259-4945-82a9-11a988ca03e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in nltk.pos_tag(tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4f45f-cf0c-45bc-9d63-3f9c6ecee2bc",
   "metadata": {},
   "source": [
    "Combien d’unités lexicales avons-nous maintenant ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8684551a-b361-4073-8b28-c55fe518a464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1149"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72564f42-36e9-46e9-9001-a9ca57137453",
   "metadata": {},
   "source": [
    "Supprimons les mots vides et la ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10b2415-67c4-43a1-b78a-0747d46da757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation\n",
    "tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "# Stopwords\n",
    "tokens = [token for token in tokens if token not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e7148-35e8-4966-8753-7413fb5f0607",
   "metadata": {},
   "source": [
    "Nous avons probablement éliminé beaucoup d’éléments inutiles ! Voyons combien d’unités lexicales demeurent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "937fcc38-13b6-49ce-836f-8aefcd644519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e89ad-16b0-42f3-867d-b8123767eb22",
   "metadata": {},
   "source": [
    "Nous pouvons provisoirement regrouper les unités restantes pour retrouver un texte lisible. Il s’agit juste de voir à quoi ressemble un résumé de livre filtré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a1e4e29-54e6-416c-a613-a273f0b864fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alex teenager living england lead gang nightly orgy opportunistic random alex friend droogs novel slang nadsat dim bruiser gang muscle georgie ambitious pete mostly play along droogs indulge taste characterize sociopath harden juvenile delinquent alex also intelligent sophisticated taste music particularly fond beethoven lovely ludwig van novel begin droogs sit favorite hangout korova milkbar drink cocktail call hype night mayhem assault scholar walk home public library rob store leave owner wife bloody unconscious stomp panhandling derelict scuffle rival gang joyride countryside stolen car break isolated cottage maul young couple living beat husband rap wife metafictional touch husband writer work manuscript call clockwork orange alex contemptuously read paragraph state novel main theme shred manuscript back milk bar alex punishes dim crude behaviour strain within gang become apparent home dreary flat alex play classical music top volume fantasizing even orgiastic violence alex skips school next day follow unexpected visit deltoid advisor alex meet pair girl take back parent flat administer hard drug rap even alex find droogs mutinous mood georgie challenge alex leadership gang demand pull job alex quell rebellion slash dim hand fighting georgie show generosity take bar alex insists follow georgie idea burgle home wealthy old woman start farce end tragic pathos alex attack kill elderly woman escape block dim attack alex leave incapacitate front step police arrive sentence prison murder alex get job wing chapel play religious music stereo service well singing hymn prison chaplain mistake alex bible study stirring faith alex actually reading scripture violent passage alex fellow cellmates blame beat troublesome cellmate death agree undergo experimental treatment call ludovico technique technique form aversion therapy alex receive injection make feel sick watch graphically violent film eventually condition suffer cripple bout nausea mere thought violence unintended consequence soundtrack one fifth alex unable listen beloved classical music effectiveness technique demonstrate group vip watch alex collapse walloping bully abase young woman whose presence arouse predatory sexual inclination though prison chaplain accuse state strip alex free government official scene please result alex release society since parent rent room lodger alex wanders street enters public library hop learn painless way commit suicide accidentally encounter old scholar assault earlier book keen revenge beat alex help friend policeman come alex rescue turn none dim former gang rival billyboy two policeman take alex outside town beat daze bloody alex collapse door isolated cottage realize late house droogs invade first half story gang wear mask assault writer recognize alex writer whose name reveal alexander shelter alex question conditioning sequence reveal alexander die injury inflict husband decide continue live fragrant memory persists despite horrid memory alexander critic government hop use alex symbol state brutality thereby prevent incumbent government eventually begin realize alex role happening night two year ago one alexander radical associate manages extract confession alex remove alexander home lock flatblock near former home alex subject relentless barrage classical music prompt attempt suicide leap high window alex wake hospital court government official anxious counter bad publicity create suicide attempt alexander safely pack mental institution alex offer job agree side government photographer snap picture alex daydream orgiastic violence realize ludovico conditioning reverse cure right final chapter alex new trio droogs find begin outgrow taste violence chance encounter pete married settle inspires alex seek wife family contemplate likelihood future son delinquent prospect alex view fatalistically'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810f6aa-a7b2-40c7-a6e2-253e113631e8",
   "metadata": {},
   "source": [
    "Il semblerait que ça ait fonctionné. Voilà une belle salade de mots. Cela ressemble à un mélange de mots difficile à déchiffrer pour un humain. Mais cela est nettement plus facile pour un ordinateur car toute apparence de grammaire a disparu.\n",
    "\n",
    "Nous pourrions aussi choisir d’ignorer les noms mais ne compliquons pas trop les choses ! Toutefois, si cela vous intéresse, recherchez sur Internet NLTK et NER (reconnaisseur d’entités nommées) à l’aide de votre moteur de recherche préféré.\n",
    "\n",
    "Nous pouvons maintenant le faire avec tous les livres. Nous intégrerons tout ce qui précède dans une seule fonction que nous appliquerons par la suite à toutes les lignes. Cela prendra quelques minutes car il y a beaucoup de livres (9292) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41a6fb42-af3d-48d2-ade4-bc277068d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(text):\n",
    "    # Make it all lower case and remove contactions\n",
    "    text = contractions.fix(text.lower())\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in nltk.pos_tag(tokens)]\n",
    "\n",
    "    # Punctuation\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92a1e13a-60ea-4a33-92e3-cee8de917c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the prepare function to all of the books and store it in a new column.\n",
    "books['prepared'] = books['summary'].apply(prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17288170-3b47-4603-b476-ef975f4499be",
   "metadata": {},
   "source": [
    "L’étape précédente est chronophage. Dans pareil cas, vous gagneriez sans doute à regrouper toutes les données dans une fichier que vous pourrez ensuite lire sans devoir exécuter à nouveau toutes les étapes préparatoires. Pour ce faire, vous pouvez utiliser les modules « _pickle » et « read_pickle ». Pickle est un module propre à Python qui est dense et rapide. Il ne se limite pas non plus aux trames de données. Toute variable de Python peut être stockée sous pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ada487fe-3df0-4168-91ec-babf9a1c47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prepared DataFrame to file\n",
    "books.to_pickle('books.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e252b60-2fce-4b68-a80c-5b52df3d3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the prepared Data Frame\n",
    "books = pd.read_pickle('books.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6a59b-7511-4974-8005-a3dae242e1c1",
   "metadata": {},
   "source": [
    "Il y a maintenant une nouvelle colonne dans notre trame de données sur les livres (`books`) qui contient le texte épuré sous forme de liste d’unités lexicales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c90ca027-0f0a-4c27-b310-b4a4f9f26ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wikipedia</th>\n",
       "      <th>freebase</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publicationdate</th>\n",
       "      <th>genres</th>\n",
       "      <th>summary</th>\n",
       "      <th>prepared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>843</td>\n",
       "      <td>/m/0k36</td>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Anthony Burgess</td>\n",
       "      <td>1962</td>\n",
       "      <td>{\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "      <td>[alex, teenager, living, england, lead, gang, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>986</td>\n",
       "      <td>/m/0ldx</td>\n",
       "      <td>The Plague</td>\n",
       "      <td>Albert Camus</td>\n",
       "      <td>1947</td>\n",
       "      <td>{\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "      <td>[text, plague, divide, five, part, town, oran,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2152</td>\n",
       "      <td>/m/0x5g</td>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>Erich Maria Remarque</td>\n",
       "      <td>1929-01-29</td>\n",
       "      <td>{\"/m/098tmk\": \"War novel\", \"/m/016lj8\": \"Roman...</td>\n",
       "      <td>The book tells the story of Paul Bäumer, a Ge...</td>\n",
       "      <td>[book, tell, story, paul, bäumer, german, sold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2890</td>\n",
       "      <td>/m/011zx</td>\n",
       "      <td>A Wizard of Earthsea</td>\n",
       "      <td>Ursula K. Le Guin</td>\n",
       "      <td>1968</td>\n",
       "      <td>{\"/m/0dwly\": \"Children's literature\", \"/m/01hm...</td>\n",
       "      <td>Ged is a young boy on Gont, one of the larger...</td>\n",
       "      <td>[ged, young, boy, go, one, large, island, nort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4081</td>\n",
       "      <td>/m/01b4w</td>\n",
       "      <td>Blade Runner 3: Replicant Night</td>\n",
       "      <td>K. W. Jeter</td>\n",
       "      <td>1996-10-01</td>\n",
       "      <td>{\"/m/06n90\": \"Science Fiction\", \"/m/014dfn\": \"...</td>\n",
       "      <td>Living on Mars, Deckard is acting as a consul...</td>\n",
       "      <td>[live, mar, deckard, act, consultant, movie, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16548</th>\n",
       "      <td>36372465</td>\n",
       "      <td>/m/02vqwsp</td>\n",
       "      <td>The Third Lynx</td>\n",
       "      <td>Timothy Zahn</td>\n",
       "      <td>2007</td>\n",
       "      <td>{\"/m/06n90\": \"Science Fiction\"}</td>\n",
       "      <td>The story starts with former government agent...</td>\n",
       "      <td>[story, start, former, government, agent, fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16550</th>\n",
       "      <td>36534061</td>\n",
       "      <td>/m/072y44</td>\n",
       "      <td>Remote Control</td>\n",
       "      <td>Andy McNab</td>\n",
       "      <td>1997</td>\n",
       "      <td>{\"/m/01jfsb\": \"Thriller\", \"/m/02xlf\": \"Fiction...</td>\n",
       "      <td>The series follows the character of Nick Ston...</td>\n",
       "      <td>[series, follow, character, nick, stone, man, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16554</th>\n",
       "      <td>37054020</td>\n",
       "      <td>/m/04f1nbs</td>\n",
       "      <td>Transfer of Power</td>\n",
       "      <td>Vince Flynn</td>\n",
       "      <td>2000-06-01</td>\n",
       "      <td>{\"/m/01jfsb\": \"Thriller\", \"/m/02xlf\": \"Fiction\"}</td>\n",
       "      <td>The reader first meets Rapp while he is doing...</td>\n",
       "      <td>[reader, first, meet, rapp, covert, operation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16555</th>\n",
       "      <td>37122323</td>\n",
       "      <td>/m/0n5236t</td>\n",
       "      <td>Decoded</td>\n",
       "      <td>Jay-Z</td>\n",
       "      <td>2010-11-16</td>\n",
       "      <td>{\"/m/0xdf\": \"Autobiography\"}</td>\n",
       "      <td>The book follows very rough chronological ord...</td>\n",
       "      <td>[book, follow, rough, chronological, order, sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16557</th>\n",
       "      <td>37159503</td>\n",
       "      <td>/m/073nkd</td>\n",
       "      <td>Poor Folk</td>\n",
       "      <td>Fyodor Dostoyevsky</td>\n",
       "      <td>1846</td>\n",
       "      <td>{\"/m/02ql9\": \"Epistolary novel\", \"/m/014dfn\": ...</td>\n",
       "      <td>Makar Devushkin and Varvara Dobroselova are s...</td>\n",
       "      <td>[makar, devushkin, varvara, dobroselova, secon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9292 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       wikipedia    freebase                            title  \\\n",
       "0            843     /m/0k36               A Clockwork Orange   \n",
       "1            986     /m/0ldx                       The Plague   \n",
       "4           2152     /m/0x5g   All Quiet on the Western Front   \n",
       "5           2890    /m/011zx             A Wizard of Earthsea   \n",
       "7           4081    /m/01b4w  Blade Runner 3: Replicant Night   \n",
       "...          ...         ...                              ...   \n",
       "16548   36372465  /m/02vqwsp                   The Third Lynx   \n",
       "16550   36534061   /m/072y44                   Remote Control   \n",
       "16554   37054020  /m/04f1nbs                Transfer of Power   \n",
       "16555   37122323  /m/0n5236t                          Decoded   \n",
       "16557   37159503   /m/073nkd                        Poor Folk   \n",
       "\n",
       "                     author publicationdate  \\\n",
       "0           Anthony Burgess            1962   \n",
       "1              Albert Camus            1947   \n",
       "4      Erich Maria Remarque      1929-01-29   \n",
       "5         Ursula K. Le Guin            1968   \n",
       "7               K. W. Jeter      1996-10-01   \n",
       "...                     ...             ...   \n",
       "16548          Timothy Zahn            2007   \n",
       "16550            Andy McNab            1997   \n",
       "16554           Vince Flynn      2000-06-01   \n",
       "16555                 Jay-Z      2010-11-16   \n",
       "16557    Fyodor Dostoyevsky            1846   \n",
       "\n",
       "                                                  genres  \\\n",
       "0      {\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...   \n",
       "1      {\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...   \n",
       "4      {\"/m/098tmk\": \"War novel\", \"/m/016lj8\": \"Roman...   \n",
       "5      {\"/m/0dwly\": \"Children's literature\", \"/m/01hm...   \n",
       "7      {\"/m/06n90\": \"Science Fiction\", \"/m/014dfn\": \"...   \n",
       "...                                                  ...   \n",
       "16548                    {\"/m/06n90\": \"Science Fiction\"}   \n",
       "16550  {\"/m/01jfsb\": \"Thriller\", \"/m/02xlf\": \"Fiction...   \n",
       "16554   {\"/m/01jfsb\": \"Thriller\", \"/m/02xlf\": \"Fiction\"}   \n",
       "16555                       {\"/m/0xdf\": \"Autobiography\"}   \n",
       "16557  {\"/m/02ql9\": \"Epistolary novel\", \"/m/014dfn\": ...   \n",
       "\n",
       "                                                 summary  \\\n",
       "0       Alex, a teenager living in near-future Englan...   \n",
       "1       The text of The Plague is divided into five p...   \n",
       "4       The book tells the story of Paul Bäumer, a Ge...   \n",
       "5       Ged is a young boy on Gont, one of the larger...   \n",
       "7       Living on Mars, Deckard is acting as a consul...   \n",
       "...                                                  ...   \n",
       "16548   The story starts with former government agent...   \n",
       "16550   The series follows the character of Nick Ston...   \n",
       "16554   The reader first meets Rapp while he is doing...   \n",
       "16555   The book follows very rough chronological ord...   \n",
       "16557   Makar Devushkin and Varvara Dobroselova are s...   \n",
       "\n",
       "                                                prepared  \n",
       "0      [alex, teenager, living, england, lead, gang, ...  \n",
       "1      [text, plague, divide, five, part, town, oran,...  \n",
       "4      [book, tell, story, paul, bäumer, german, sold...  \n",
       "5      [ged, young, boy, go, one, large, island, nort...  \n",
       "7      [live, mar, deckard, act, consultant, movie, c...  \n",
       "...                                                  ...  \n",
       "16548  [story, start, former, government, agent, fran...  \n",
       "16550  [series, follow, character, nick, stone, man, ...  \n",
       "16554  [reader, first, meet, rapp, covert, operation,...  \n",
       "16555  [book, follow, rough, chronological, order, sw...  \n",
       "16557  [makar, devushkin, varvara, dobroselova, secon...  \n",
       "\n",
       "[9292 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8da10-7e78-491f-9f1d-0d44278181e6",
   "metadata": {},
   "source": [
    "### L'algorithme\n",
    "\n",
    "Voyons maintenant l’algorithme d’apprentissage-machine, appelé Latent Dirichlet Allocation (Allocation latente de Dirichlet ou LDA). C’est un modèle statistique capable de localiser dans un échantillon des mots qui vont souvent ensemble puis de les combiner avec tous les échantillons pour former des groupes dans lesquels certains termes sont utilisés fréquemment.\n",
    "\n",
    "L’inconvénient de cette technique non dirigée est qu’elle ne spécifiera pas la nature des catégories identifiées. Une expertise en la matière est nécessaire pour les nommer. Par ailleurs, le modèle ignore combien il y a de sujets différents. Vous devez spécifier ce nombre au préalable. Une approche essais-erreurs est donc nécessaire puisque vous devrez, en tant que personne, déterminer si les groupements effectués par l’ordinateur font du sens. Il faudra des groupes additionnels si les groupements sont trop génériques. Inversement, il faudra moins de groupes si les recoupements sont trop nombreux.\n",
    "\n",
    "Le modèle d’allocation latente de Dirichlet faisant partie du paquet gensim, nous devons tout d’abord le charger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b24b4ff3-e50c-446f-ac8b-43fbe75e6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212bc60f-b0e9-4509-9489-726cc0bb804b",
   "metadata": {},
   "source": [
    "Nous devons maintenant créer un dictionnaire à partir du texte que nous avons préparé et épuré précédemment. Ce dictionnaire contiendra tous les mots uniques identifiés dans le corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7468223c-1a1d-4398-8cf5-253cac8391bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = gensim.corpora.Dictionary(list(books['prepared']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc9fe5-6097-4857-85dc-37578415c35d",
   "metadata": {},
   "source": [
    "Nous pouvons vérifier combien de mots uniques comporte ce dictionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faced100-ffa0-4181-9e0a-555d5de3373f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76292"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131ead7-00d9-4b1f-b78e-5e441d05c0e0",
   "metadata": {},
   "source": [
    "Nous passerons ensuite à l’étape de filtrage des mots les plus répandus et des mots qui n’apparaissent que très rarement. La suppression des mots les plus répandus ressemble à l’étape de retrait des mots vides que nous avons complétée précédemment, exception faite qu’elle s’appliquera à l’ensemble du corpus. Nous découvrirons certains autres mots qui nous ont échappés précédemment.\n",
    "\n",
    "Les mots qui apparaissent rarement doivent aussi être supprimés car si un mot ne survient que quelques fois dans l’ensemble du corpus, son emploi a peu de valeur pour définir un groupe de mots qui vont ensemble. L’allocation latente de Dirichlet cherchera à constituer des groupes en fonction de mots que l’on retrouve souvent juxtaposés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc2e00c4-38df-47cb-9367-8037b9cde7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c8a391-f98b-4b46-bd17-3fb29c2fe0ef",
   "metadata": {},
   "source": [
    "Nous devons maintenant faire une chose un peu étrange. Nous devons porter attention au premier élément (ou en fait n’importe lequel !) du dictionnaire des mots communs `common_dictionary` tout juste filtré. Python sera ainsi contraint de mettre le dictionnaire en mémoire. Si vous omettez cette étape, la fonction `id2token` ne fonctionnera pas le moment venu d’entrainer le modèle. Cette fonction permet de traduire les identifiants des mots contenus dans le dictionnaire en mots réels ce qui facilite l’interprétation. À titre d’exemple, un ordinateur aime gérer des identifiants numériques \\[1,4,3\\], où 1 pourrait signifier « how », 4 « are » et 3 « you ». Comme de telles données sont difficiles à gérer pour un humain, la fonction `id2token` traduira automatiquement \\[1,4,3\\] en \\[\"how\",\"are\",\"you\"\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "116f66ac-e245-4b71-b24b-c26e3fa79256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accidentally'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_dictionary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10162a37-46a1-4f16-b47e-d8daf79aa309",
   "metadata": {},
   "source": [
    "Nous disposons maintenant d’un dictionnaire filtré que nous pouvons utiliser pour transformer notre corpus de résumés de livres sous un format que peut comprendre l’algorithme d’allocation latente de Dirichlet. Nous devons utiliser la fonction `doc2bow` pour transformer les résumés de livres (documents) en un sac de mots. Un sac de mots est simplement une liste de mots dans lequel est spécifié le nombre de fois qu’un mot est utilisé dans le document. Mais plutôt qu’utiliser le mot, c’est l’identifiant du mot apparaissant dans le dictionnaire qui sera utilisé, une approche beaucoup plus rapide et plus efficiente en termes de mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5278ddc1-489f-4dbe-890e-5e14a697f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus = [common_dictionary.doc2bow(text) for text in books['prepared']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f839e8-c183-4d10-85b3-3b29fbb0beb6",
   "metadata": {},
   "source": [
    "Nous voulons à ce stade activer la journalisation. Cela nous permettra de voir ce que fait le modèle d’allocation latente de Dirichlet pendant son entrainement. Nous aurions autrement affaire à une boîte noire. Nous dirons à Python de tenir un journal dans « lda_model.log » et de tout conserver, des simples messages de mise au point en allant vers le haut.  « Vers le haut » signifie ici de stocker les messages de mise au point, les messages d’avertissement et les messages d’erreurs.\n",
    "\n",
    "Nous pourrons ouvrir ce fichier ultérieurement pour voir tout ce que le modèle a accompli et déterminer s’il a fait ou non du bon boulot, et si certains paramètres doivent être ajustés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24aed76f-0166-466b-8fd8-744c9adbadd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', filemode='w', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d3f4a-8aaa-491d-b078-98ad4a639202",
   "metadata": {},
   "source": [
    "Tout est enfin prêt pour que nous passions à l’entrainement. Nous devons cependant régler les paramètres du modèle ! Il y a beaucoup de choses que vous pouvez indiquer au modèle d’allocation latente de Dirichlet quant au mode d’entrainement. Spécifier le nombre de sujets est sans aucun doute le plus important.\n",
    "\n",
    "Nous devons indiquer avant tout au modèle d’allocation latente de Dirichlet le nombre de groupes qu’il devra créer. Dans ce cas-ci, disons 10 groupes, un nombre ni trop élevé ni trop bas, sans aucune justification. Le nombre de groupes est fonction de vos attentes. Dix genres semblent une estimation raisonnable compte tenu des livres dont nous disposons. En fait, cela dépend du nombre de livres dans le corpus. Nous ne voudrions pas 9 292 groupes au risque de se retrouver avec un seul livre par groupe. Nous ne voulons pas non plus 2 groupes car il y a certainement plus de 2 genres de livres. Il faut toujours faire preuve de jugement, selon les attentes.\n",
    "\n",
    "Nous fixons aussi `eval_every` à `1` pour que quelque chose soit consignée dans le journal à chaque passage. Ici un passage veut dire que le modèle examinera la distance entre les mots dans chaque bloc, tentera de réduire la distance entre les mots qui se retrouvent souvent ensemble, et écartera les autres mots plus loin pour constituer des groupes. Habituellement, il serait plus efficace sur le plan informatique de procéder à une évaluation sur un nombre donné de passages, mais nous voulons dans ce cas-ci évaluer chaque passage pour mieux juger de la qualité du modèle.  \n",
    "\n",
    "Le nombre de passages (`passes`) indique à l’algorithme combien de fois il devra répéter l’entrainement. Le modèle deviendra à chaque passage plus apte à trouver des connexions entre les mots et donc à former des groupes. En effet, le modèle d’allocation latente de Dirichlet tente de minimiser la « distance entre les mots » au sein de groupes tout en optimisant les mots dans d’autres groupes. Il permute lentement toutes ces informations en réduisant de plus en plus les laissés pour compte. Nous pourrons observer dans le journal son niveau de rendement. Nous nous limiterons pour l’instant à 20 passages et verrons le résultat.\n",
    "\n",
    "Il y a aussi `chunksize`, c’est-à-dire la taille des blocs, qui indique à l’algorithme le nombre de livres à traiter simultanément. Un nombre élevé lui permettra d’identifier plus rapidement davantage de corrélations, au détriment d’une plus grande puissance de calcul.\n",
    "\n",
    "Enfin, la fonction `random_state` assure que vous obteniez la même réponse après chaque entrainement fait avec les paramètres ci-dessus. Elle fixe la valeur de départ du générateur de nombres aléatoires qui produira toujours la même séquence de nombres « aléatoires » utilisée par le modèle. Tout devient alors reproductible. Nous avons choisi le nombre `42` et pourquoi pas ! Tout ce qui importe c’est que le nombre soit fixe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f54b2-d07c-45c9-8875-dbc306cb5d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2677b15e-319b-47e7-8d3a-00ef0399310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=common_corpus,\n",
    "    num_topics=10,\n",
    "    id2word=common_dictionary.id2token,\n",
    "    eval_every=1,\n",
    "    passes=20,\n",
    "    chunksize=3000,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eba169-255d-4658-bfee-e86d525bbaa4",
   "metadata": {},
   "source": [
    "Notre modèle est donc en cours d’entrainement. Ouvrons le fichier « lda_model.log » et cherchons les lignes où l’on trouve l’énoncé suivant : \n",
    "```\n",
    "DEBUG : 1453/3000 documents converged within 50 iterations\n",
    "```\n",
    "Ce nombre doit être le plus élevé possible. Idéalement, nous souhaitons la convergence de tous les documents, sachant que nous n’y arriverons jamais puisque les groupes seront imparfaits car les résumés de livres traiteront de nombreux sujets et utiliseront parfois un langage commun.\n",
    "\n",
    "Une fois le modèle entrainé, il serait préférable de le sauvegarder dans un fichier.  Ainsi, nous n’aurons pas à faire un nouvel entrainement chaque fois que nous ouvrirons une nouvelle session Jupyter. Il nous suffira de charger le modèle déjà entrainé. Vous pourrez aussi l’ouvrir dans un autre carnet axé sur l’utilisation de ce modèle ou le partager avec d’autres chercheurs.\n",
    "\n",
    "Nous avons utilisé un peu plus tôt dans ce carnet le module pickle pour stocker efficacement notre jeu de données préparées. Mais pour partager des modèles, il est préférable d’utiliser un format de données plus normalisé puisque le format pickle dépend de la version de Python et même de l’ordinateur que vous utilisez. Utiliser des formats de données standards compte parmi les meilleures pratiques de gestion des données et accroit l’interopérabilité et la reproductibilité dans le domaine de la recherche.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb106aa3-dee5-472c-9dc2-b44968c83268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "lda.save('bookmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "470b1d84-2a30-4e4e-ba7e-c927633f750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "lda = gensim.models.ldamodel.LdaModel.load('bookmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a814e-78d5-43bc-9da2-ef6efb509bea",
   "metadata": {},
   "source": [
    "### Analyse\n",
    "\n",
    "Le temps est venu d’analyser les résultats. Nous pouvons analyser plusieurs choses.  Nous pouvons, pour les 10 thématiques que nous avons demandées au modèle d’allocation latente de Dirichlet de trouver, imprimer les mots utilisés et voir dans quelle mesure ils appartiennent à un genre de livre spécifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86afbcb6-d1fc-49d1-8751-2ca8083d6c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.011*\"murder\" + 0.009*\"kill\" + 0.009*\"go\" + 0.008*\"police\" + 0.008*\"tell\" + 0.007*\"get\" + 0.006*\"case\" + 0.005*\"call\" + 0.005*\"day\" + 0.005*\"death\"')\n",
      "(1, '0.009*\"kill\" + 0.008*\"king\" + 0.007*\"return\" + 0.005*\"help\" + 0.005*\"use\" + 0.005*\"attack\" + 0.005*\"tell\" + 0.005*\"world\" + 0.005*\"back\" + 0.005*\"power\"')\n",
      "(2, '0.011*\"go\" + 0.008*\"get\" + 0.008*\"tell\" + 0.007*\"make\" + 0.006*\"leave\" + 0.006*\"see\" + 0.006*\"come\" + 0.006*\"island\" + 0.005*\"back\" + 0.005*\"say\"')\n",
      "(3, '0.017*\"novel\" + 0.016*\"book\" + 0.012*\"story\" + 0.008*\"character\" + 0.007*\"life\" + 0.006*\"world\" + 0.005*\"also\" + 0.005*\"first\" + 0.005*\"chapter\" + 0.005*\"include\"')\n",
      "(4, '0.019*\"john\" + 0.012*\"four\" + 0.012*\"go\" + 0.011*\"thomas\" + 0.011*\"henry\" + 0.011*\"sam\" + 0.008*\"luke\" + 0.008*\"tell\" + 0.007*\"sarah\" + 0.007*\"time\"')\n",
      "(5, '0.012*\"ship\" + 0.011*\"human\" + 0.010*\"earth\" + 0.009*\"planet\" + 0.007*\"time\" + 0.006*\"new\" + 0.006*\"world\" + 0.005*\"use\" + 0.005*\"year\" + 0.005*\"destroy\"')\n",
      "(6, '0.011*\"family\" + 0.010*\"father\" + 0.010*\"mother\" + 0.008*\"love\" + 0.008*\"life\" + 0.008*\"become\" + 0.008*\"child\" + 0.007*\"leave\" + 0.006*\"friend\" + 0.006*\"year\"')\n",
      "(7, '0.011*\"richard\" + 0.010*\"de\" + 0.009*\"max\" + 0.008*\"jake\" + 0.007*\"man\" + 0.007*\"hugh\" + 0.007*\"return\" + 0.007*\"leave\" + 0.007*\"tell\" + 0.006*\"day\"')\n",
      "(8, '0.011*\"war\" + 0.007*\"force\" + 0.007*\"kill\" + 0.006*\"army\" + 0.005*\"attempt\" + 0.005*\"attack\" + 0.005*\"state\" + 0.005*\"bond\" + 0.004*\"order\" + 0.004*\"use\"')\n",
      "(9, '0.013*\"harry\" + 0.012*\"get\" + 0.012*\"school\" + 0.010*\"jack\" + 0.009*\"go\" + 0.008*\"sydney\" + 0.008*\"ben\" + 0.006*\"work\" + 0.006*\"team\" + 0.006*\"name\"')\n"
     ]
    }
   ],
   "source": [
    "for t in lda.print_topics(num_words=10):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd1bc7-6c6f-4457-9466-36e9f9801b79",
   "metadata": {},
   "source": [
    "Le premier semble très axé sur le crime avec des mots comme murder, kill, police et case, mais comme vous pouvez le constater, certains mots semblent hors contexte.  Par exemple, la thématique numéro 5 tend vers la science-fiction avec des mots comme human, earth, planet et world.\n",
    "\n",
    "Mais dans d’autres cas, il n’est pas toujours évident de déterminer en quoi ce sont des genres différents. Par exemple, on retrouve essentiellement des noms à la thématique 4. N’oubliez pas que c’est nous qui cherchons différents genres mais ce n’est peut-être pas ce que l’algorithme d’allocation latente de Dirichlet a trouvé. Il cherchait simplement une manière quelconque de regrouper ces résumés de livres en 10 thématiques.\n",
    "\n",
    "Il faut aussi examiner les principaux mots par thématique. Cela ressemble à la fonction `print_topics` mais nous recherchons ici la cohérence. La cohérence dans gensim est un paramètre qui note la « distance » entre les mots pour une thématique donnée. Plus le taux de cohérence est élevé, mieux c’est."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6d0088d-4ed4-485a-a51e-bee18ecc6984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.0110494755, 'family'),\n",
       "   (0.010247785, 'father'),\n",
       "   (0.009740395, 'mother'),\n",
       "   (0.008020293, 'love'),\n",
       "   (0.007960088, 'life'),\n",
       "   (0.007780727, 'become'),\n",
       "   (0.007565873, 'child'),\n",
       "   (0.007225185, 'leave'),\n",
       "   (0.006368225, 'friend'),\n",
       "   (0.0059221974, 'year'),\n",
       "   (0.005589299, 'go'),\n",
       "   (0.0055839773, 'young'),\n",
       "   (0.005487799, 'return'),\n",
       "   (0.0051857894, 'live'),\n",
       "   (0.0051689884, 'home'),\n",
       "   (0.0051486026, 'new'),\n",
       "   (0.004815906, 'meet'),\n",
       "   (0.004654479, 'daughter'),\n",
       "   (0.0044921557, 'marry'),\n",
       "   (0.004474495, 'make')],\n",
       "  -0.852357147559029),\n",
       " ([(0.009047911, 'kill'),\n",
       "   (0.0076246485, 'king'),\n",
       "   (0.006640664, 'return'),\n",
       "   (0.0054942826, 'help'),\n",
       "   (0.0052796644, 'use'),\n",
       "   (0.005254651, 'attack'),\n",
       "   (0.0051531107, 'tell'),\n",
       "   (0.0051329145, 'world'),\n",
       "   (0.004670613, 'back'),\n",
       "   (0.004652856, 'power'),\n",
       "   (0.0043452256, 'make'),\n",
       "   (0.004283357, 'leave'),\n",
       "   (0.004097437, 'go'),\n",
       "   (0.0039577885, 'magic'),\n",
       "   (0.0037244798, 'become'),\n",
       "   (0.0037072925, 'give'),\n",
       "   (0.0036736762, 'also'),\n",
       "   (0.0036432738, 'order'),\n",
       "   (0.0035776694, 'see'),\n",
       "   (0.0035167304, 'force')],\n",
       "  -0.8798599979278723),\n",
       " ([(0.011066067, 'murder'),\n",
       "   (0.009275636, 'kill'),\n",
       "   (0.00888664, 'go'),\n",
       "   (0.00845129, 'police'),\n",
       "   (0.00800952, 'tell'),\n",
       "   (0.00742133, 'get'),\n",
       "   (0.005969997, 'case'),\n",
       "   (0.005172954, 'call'),\n",
       "   (0.0051046726, 'day'),\n",
       "   (0.0050141024, 'death'),\n",
       "   (0.0050025163, 'later'),\n",
       "   (0.0048860894, 'say'),\n",
       "   (0.004859923, 'leave'),\n",
       "   (0.00480853, 'time'),\n",
       "   (0.00438591, 'back'),\n",
       "   (0.0043737623, 'discover'),\n",
       "   (0.004306422, 'man'),\n",
       "   (0.0042611184, 'try'),\n",
       "   (0.004226807, 'give'),\n",
       "   (0.0041537397, 'body')],\n",
       "  -0.8994934725642094),\n",
       " ([(0.010877573, 'go'),\n",
       "   (0.008306963, 'get'),\n",
       "   (0.0077550416, 'tell'),\n",
       "   (0.0066984645, 'make'),\n",
       "   (0.0059883306, 'leave'),\n",
       "   (0.0058463197, 'see'),\n",
       "   (0.0058085267, 'come'),\n",
       "   (0.00555334, 'island'),\n",
       "   (0.005428566, 'back'),\n",
       "   (0.0053745257, 'say'),\n",
       "   (0.005361099, 'boy'),\n",
       "   (0.0051668733, 'man'),\n",
       "   (0.0048363074, 'call'),\n",
       "   (0.004833694, 'cat'),\n",
       "   (0.004583741, 'two'),\n",
       "   (0.0045204535, 'day'),\n",
       "   (0.0043924847, 'little'),\n",
       "   (0.00428675, 'old'),\n",
       "   (0.004215559, 'book'),\n",
       "   (0.0041349875, 'way')],\n",
       "  -1.1089524416940162),\n",
       " ([(0.016524425, 'novel'),\n",
       "   (0.01635563, 'book'),\n",
       "   (0.011665621, 'story'),\n",
       "   (0.0078525655, 'character'),\n",
       "   (0.007079506, 'life'),\n",
       "   (0.0058026775, 'world'),\n",
       "   (0.005374613, 'also'),\n",
       "   (0.004859394, 'first'),\n",
       "   (0.0046915784, 'chapter'),\n",
       "   (0.0046431622, 'include'),\n",
       "   (0.004371545, 'end'),\n",
       "   (0.004213073, 'describe'),\n",
       "   (0.0041669966, 'time'),\n",
       "   (0.0041174893, 'write'),\n",
       "   (0.0040176734, 'people'),\n",
       "   (0.0039546695, 'become'),\n",
       "   (0.0038098574, 'part'),\n",
       "   (0.0037996897, 'event'),\n",
       "   (0.003589654, 'work'),\n",
       "   (0.0035832277, 'set')],\n",
       "  -1.1586394257033297),\n",
       " ([(0.011816035, 'ship'),\n",
       "   (0.0106709255, 'human'),\n",
       "   (0.009998546, 'earth'),\n",
       "   (0.008778745, 'planet'),\n",
       "   (0.006789669, 'time'),\n",
       "   (0.0057856753, 'new'),\n",
       "   (0.005676125, 'world'),\n",
       "   (0.0052419775, 'use'),\n",
       "   (0.0047859857, 'year'),\n",
       "   (0.0047349636, 'destroy'),\n",
       "   (0.0046564755, 'system'),\n",
       "   (0.0044906987, 'alien'),\n",
       "   (0.0044494793, 'crew'),\n",
       "   (0.0042965407, 'two'),\n",
       "   (0.0040266784, 'space'),\n",
       "   (0.0038974553, 'discover'),\n",
       "   (0.0038509928, 'become'),\n",
       "   (0.0036873547, 'race'),\n",
       "   (0.0035617596, 'force'),\n",
       "   (0.0034394104, 'group')],\n",
       "  -1.2880320281603785),\n",
       " ([(0.019350901, 'john'),\n",
       "   (0.012343307, 'four'),\n",
       "   (0.011951407, 'go'),\n",
       "   (0.011256119, 'thomas'),\n",
       "   (0.010943421, 'henry'),\n",
       "   (0.010530382, 'sam'),\n",
       "   (0.007921846, 'luke'),\n",
       "   (0.0075592403, 'tell'),\n",
       "   (0.0072635054, 'sarah'),\n",
       "   (0.0066445973, 'time'),\n",
       "   (0.0066089514, 'six'),\n",
       "   (0.006549933, 'mark'),\n",
       "   (0.006211759, 'back'),\n",
       "   (0.0060505266, 'say'),\n",
       "   (0.005687484, 'escape'),\n",
       "   (0.005570402, 'see'),\n",
       "   (0.0054911813, 'get'),\n",
       "   (0.0054528024, 'number'),\n",
       "   (0.0053912117, 'also'),\n",
       "   (0.005212229, 'try')],\n",
       "  -1.6273870567537805),\n",
       " ([(0.010955997, 'war'),\n",
       "   (0.007185829, 'force'),\n",
       "   (0.006688412, 'kill'),\n",
       "   (0.0055585653, 'army'),\n",
       "   (0.005357544, 'attempt'),\n",
       "   (0.005083893, 'attack'),\n",
       "   (0.005018706, 'state'),\n",
       "   (0.004784857, 'bond'),\n",
       "   (0.0043996433, 'order'),\n",
       "   (0.004351693, 'use'),\n",
       "   (0.0042293402, 'officer'),\n",
       "   (0.004001528, 'plan'),\n",
       "   (0.0038744204, 'agent'),\n",
       "   (0.00381393, 'lead'),\n",
       "   (0.0037828526, 'british'),\n",
       "   (0.0036342563, 'united'),\n",
       "   (0.003631425, 'general'),\n",
       "   (0.0034998427, 'government'),\n",
       "   (0.0034448202, 'american'),\n",
       "   (0.0034325018, 'send')],\n",
       "  -1.6469919357467118),\n",
       " ([(0.012514268, 'harry'),\n",
       "   (0.011745077, 'get'),\n",
       "   (0.01160046, 'school'),\n",
       "   (0.010107191, 'jack'),\n",
       "   (0.009295834, 'go'),\n",
       "   (0.0084613105, 'sydney'),\n",
       "   (0.0075095803, 'ben'),\n",
       "   (0.006206516, 'work'),\n",
       "   (0.005804462, 'team'),\n",
       "   (0.0056178784, 'name'),\n",
       "   (0.0054714642, 'see'),\n",
       "   (0.0052048955, 'back'),\n",
       "   (0.0050857826, 'call'),\n",
       "   (0.0050535523, 'meet'),\n",
       "   (0.005051035, 'time'),\n",
       "   (0.0049523804, 'make'),\n",
       "   (0.0049423566, 'rise'),\n",
       "   (0.004920191, 'adrian'),\n",
       "   (0.004907922, 'tell'),\n",
       "   (0.004893738, 'book')],\n",
       "  -1.6768336028652775),\n",
       " ([(0.011130674, 'richard'),\n",
       "   (0.009534553, 'de'),\n",
       "   (0.008858717, 'max'),\n",
       "   (0.0075008064, 'jake'),\n",
       "   (0.007250416, 'man'),\n",
       "   (0.007025279, 'hugh'),\n",
       "   (0.006880192, 'return'),\n",
       "   (0.0065772273, 'leave'),\n",
       "   (0.0065650116, 'tell'),\n",
       "   (0.0063807275, 'day'),\n",
       "   (0.0061049056, 'father'),\n",
       "   (0.005954069, 'abbey'),\n",
       "   (0.005862775, 'brother'),\n",
       "   (0.0057803867, 'rachel'),\n",
       "   (0.0053491513, 'two'),\n",
       "   (0.0052772104, 'back'),\n",
       "   (0.0052246293, 'sir'),\n",
       "   (0.0051900796, 'see'),\n",
       "   (0.0050810473, 'come'),\n",
       "   (0.0045155785, 'go')],\n",
       "  -1.9037630489461803)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.top_topics(common_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1e291-1f2f-4f2b-8431-cfca3eb292d8",
   "metadata": {},
   "source": [
    "Voyons à quels groupes le modèle d’allocation latente de Dirichlet juge que notre premier livre d’index 0, « A Clockwork Orange », appartient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee8abd8c-2e70-43e1-a589-4537b8952a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Clockwork Orange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.44291747), (3, 0.26078534), (6, 0.21150394), (8, 0.07269716)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(books.iloc[0].title)\n",
    "lda[common_corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab61522-12ff-41a0-ab32-f904a6019356",
   "metadata": {},
   "source": [
    "Les chiffres indiquent le numéro de groupe et la probabilité que ce livre appartiennent à ce groupe. Le modèle d’allocation latente de Dirichlet estime avec 44,3 % de confiance, que « A Clockwork Orange » appartient au groupe. Ce n’est pas un niveau de confiance très élevé.\n",
    "\n",
    "Nous pouvons aussi tourner notre attention vers les livres qui appartiennent très certainement à un groupe donné. Nous utiliserons pour cela la magie de Python. Nous créerons d’abord une fonction qui transforme le résultat de sortie ci-dessus en un élément plus facile à gérer, en ajoutant des zéros à tous les groupes exclus du résultat. Nous avons pour « A Clockwork Orange » les groupes 0, 3, 6 et 8. Nous créerons donc de nouvelles entrées pour les groupes manquants 1, 2, 4, 5, 7 et 9 en fixant ceux-ci à zéro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c58f67bc-6765-4988-9ac6-4c079fa805c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def densify(sparse, num_topics=10):\n",
    "    full = [0]*num_topics\n",
    "    for s in sparse:\n",
    "        full[s[0]] = s[1]\n",
    "    return full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def450eb-2f69-44fc-84a9-9011aec89f15",
   "metadata": {},
   "source": [
    "Nous utiliserons maintenant cette fonction pour convertir l’ensemble des résultats obtenus par le modèle d’allocation latente de Dirichlet pour chacun des livres en une matrice de 9292 (nombre de livres) par 10 (nombre de groupes), avant de charger le tout dans une trame de données de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "421f0f69-35f5-4158-8a68-99499e226b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicmatrix=pd.DataFrame([densify(lda[c]) for c in common_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f29aae-15be-43ef-a484-b88123a79d63",
   "metadata": {},
   "source": [
    "Nous pouvons ensuite intégrer le tout à la trame de données originales `books`. Nous devons pour cela réinitialiser l’index de la trame de données `books` car nous avions écarté préalablement de nombreux livres associés à des données manquantes. Réinitialiser l’index assurera que celui-ci fonctionne de 0 à 9291, ce qui équivaut à notre trame de données `topicmatrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26ff1529-0f41-468f-8f36-441327f3c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = books.reset_index().join(topicmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da347212-fd23-4b1f-82ce-cfe1bb8fd137",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant utiliser Pandas pour identifier les livres qui appartiennent au groupe 2, avec un niveau de confiance supérieur à 95 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7bbd474-68f0-43a7-93d6-4e1720221050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3695                           Punk Farm\n",
       "5340     The Moomins and the Great Flood\n",
       "5699    The Story of A Fierce Bad Rabbit\n",
       "5943            Five Go Off In A Caravan\n",
       "6013         Curious George Flies a Kite\n",
       "6437                          Fox's Feud\n",
       "8034                   The Missing Piece\n",
       "8095                      Curious George\n",
       "8359                 Battle for the Park\n",
       "8737                              Bedlam\n",
       "9003                     The Sly Old Cat\n",
       "9035         Curious George Gets a Medal\n",
       "9141                       The Gathering\n",
       "9150                         Troll Blood\n",
       "9210                      Little Red Cap\n",
       "9211                When the Moon Forgot\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined[joined[2] > 0.95][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f6961a-82bf-41ba-80f7-ce4d4cb805c6",
   "metadata": {},
   "source": [
    "Encore une fois, il nous revient de réfléchir à ce que ces livres peuvent avoir en commun. Si l’on se fie aux titres, la plupart des livres seraient des livres de jeunesse !\n",
    "\n",
    "Qu’obtient-on pour le groupe 1 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0193a9f-8806-4d24-bce2-a7e02c8bf28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855                             The Dragon Reborn\n",
       "1123                     The Wishsong of Shannara\n",
       "1546                         To Green Angel Tower\n",
       "2029                          The Source of Magic\n",
       "3496                                 Resurrection\n",
       "3751                          Dawn of the Dragons\n",
       "3777                                   Stronghold\n",
       "3804                       The Kingdoms of Terror\n",
       "3805                                 Castle Death\n",
       "3808                       The Dungeons of Torgar\n",
       "3896                        The Hunger of Sejanoz\n",
       "3909                                    Vampirium\n",
       "3910                   The Fall of Blood Mountain\n",
       "3963                         The Hour of the Gate\n",
       "4597                           Chosen of the Gods\n",
       "4860                         The Bone Doll's Twin\n",
       "4879                                Deryni Rising\n",
       "4966                               Rise of a Hero\n",
       "4979                                   Shadowplay\n",
       "5063                      Dark Wraith of Shannara\n",
       "5997                      Tanis, the Shadow Years\n",
       "6095                            The Eternal Flame\n",
       "6485           The Twelve Kingdoms: Sea of Shadow\n",
       "6499                          Master of the Books\n",
       "7045                    The Seven Songs of Merlin\n",
       "7403                            The Riven Kingdom\n",
       "7658                    Beyond the Nightmare Gate\n",
       "7672                           War of the Wizards\n",
       "7844                            The Kiss of Death\n",
       "8069                           Moon of the Spider\n",
       "8283           The Twelve Kingdoms: Skies of Dawn\n",
       "8857                               The White Hart\n",
       "8920    World of Warcraft: Beyond the Dark Portal\n",
       "9069                                Lord of Souls\n",
       "9095                         Rage of a Demon King\n",
       "9100                                    Whizzard!\n",
       "9119                     Talon of the Silver Hawk\n",
       "9120                                King of Foxes\n",
       "9123                               Exile's Return\n",
       "9178                      The Tower at Stony Wood\n",
       "9192                                   Storm Born\n",
       "9209                                Dragon's Oath\n",
       "9221                                     Awakened\n",
       "9223                    Infinity Blade: Awakening\n",
       "9230                                  Neverwinter\n",
       "9236                                        Darke\n",
       "9260                        Quest for Lost Heroes\n",
       "9264        Supplement to the Journey to the West\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined[joined[1] > 0.95][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adf7ce-3817-42ec-9d89-2c2e2c02c188",
   "metadata": {},
   "source": [
    "Ou de fantaisie peut-être ?\n",
    "\n",
    "Ce n’est pas toujours évident. Quelle serait par exemple la thématique du groupe 9 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b755f4d9-f0ab-4fc1-a127-23d7d25ab3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8782    Ishmael and the Return of the Dugongs\n",
       "9017                               Bloodlines\n",
       "9103                       Soccer Comes First\n",
       "9105                                     Goal\n",
       "9202                        Succubus Revealed\n",
       "9237                The Luck of Ginger Coffey\n",
       "9268                  Big Nate: Strikes Again\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined[joined[9] > 0.95][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c2cbe-0f17-454a-aa77-75e100e52f73",
   "metadata": {},
   "source": [
    "Il n’y a pas beaucoup de livres cette fois-ci. Donc même le modèle d’allocation latente de Dirichlet n’est pas trop sûr des groupes auxquels appartiennent ces livres. Lorsque cela se produit, il vous faudra peut-être peaufiner les paramètres du modèle d’allocation latente de Dirichlet pour obtenir de meilleurs résultats. Nous pouvons par exemple diminuer le nombre de groupes. L’algorithme d’apprentissage non dirigé regroupe lui-même les éléments. Il est donc très utile pour déceler des tendances mais c’est à l’humain qu’il revient en bout de ligne d’interpréter les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d864ef9-3c9f-45fc-b061-3f683b41910b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "C’est là le problème de l’apprentissage non dirigé. L’algorithme identifiera des éléments qui vont ensemble mais sans préciser pourquoi. Il peut seulement vous indiquer les mots qu’il a utilisés pour regrouper les livres mais pour le modèle, ces mots ne sont que des indexes dans un dictionnaire. Il ne voit que des chiffres et les mots n’ont aucune signification.\n",
    "\n",
    "Nous avons noté que nous pouvions, avec le jeu de données sur les livres, amener le modèle à faire des choix plutôt bons quant aux livres qui allaient ensemble. Nous avons aussi pu interpréter le nature de certains groupes, c.-à-d. le genre, en fonction de facteurs communs. Nous avons aussi appris à préparer les données destinées à alimenter les modèles de traitement automatique du langage naturel.\n",
    "\n",
    "À ce stade, vous devriez commencer à ajuster les paramètres. À titre d’exemple, peut-être avons-nous opté pour trop (ou trop peu) de thématiques. Parmi les autres paramètres, il est possible de modifier le nombre de passages lors de l’entrainement ou la taille des blocs. Le [Manuel du modèle d’allocation latente de Dirichlet (en anglais)](https://radimrehurek.com/gensim/models/ldamodel.html) recense d’autres paramètres que vous pouvez modifier mais que nous n’avons pas ajustés. Il s’agit d’atteindre un équilibre entre vos ressources informatiques et le niveau de précision. Vous devrez surveiller étroitement le fichier de consignation (journal) généré par le modèle d’allocation latente de Dirichlet pour vous assurer que le réglage de vos paramètres a l’effet voulu. \n",
    "\n",
    "Il existe de nombreux autres modèles de traitement du langage naturel que vous pourriez utiliser mais cela dépasse le cadre du présent tutoriel. À titre d’exemple, l'algorithme de [regroupement DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) mériterait qu’on s’y attarde. Vous pourriez aussi utiliser la [génération augmentée de récupération](https://fr.wikipedia.org/wiki/G%C3%A9n%C3%A9ration_augment%C3%A9e_de_r%C3%A9cup%C3%A9ration) avec un modèle de langage de grande taille, pour une approche plus interactive. Si ces deux méthodes sortent du cadre de cette série sur l’apprentissage-machine, nous vous invitons fortement à y jeter un coup d’œil !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162304c-8ca3-4a84-b73c-6d7ebff96b28",
   "metadata": {},
   "source": [
    "Références :\n",
    "- https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "- https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c1a7d-4fac-4124-9b2f-dac0575c6624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
